{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc05a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from models.lstm_cnn_attention import LSTMCNNAttention\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2cdeb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: does the model run?\n",
    "\n",
    "model = LSTMCNNAttention()\n",
    "\n",
    "x = torch.randn(8, 75, 3)  # batch of 8 samples\n",
    "y = model(x)\n",
    "\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbaaf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10500, 75, 3) (10500,)\n",
      "Val shape: (2250, 75, 3) (2250,)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-generated dataset\n",
    "X_train = np.load(\"../data/X_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "\n",
    "X_val = np.load(\"../data/X_val.npy\")\n",
    "y_val = np.load(\"../data/y_val.npy\")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Val shape:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038c9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "'''\n",
    "    Why?\n",
    "    PyTorch models only work with tensors\n",
    "    Labels must be long for classification\n",
    "'''\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype = torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype = torch.long)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype = torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f7aee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Simple. No tuning yet '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = LSTMCNNAttention()\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "''' Simple. No tuning yet '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b320a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, X, y, optimizer, criterion, batch_size):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    num_batches = (len(X) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = X[i:i + batch_size]\n",
    "        yb = y[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim = 1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Batch {i//batch_size + 1}/{num_batches} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    acc = correct / len(X)\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94fa7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop\n",
    "\n",
    "def evaluate(model, X, y, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        total_loss = loss.item()\n",
    "        preds = outputs.argmax(dim = 1)\n",
    "        correct = (preds == y).sum().item()\n",
    "\n",
    "    acc = correct / len(X)\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102c97bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/165 - Loss: 1.0947\n",
      "Batch 2/165 - Loss: 1.0827\n",
      "Batch 3/165 - Loss: 1.0729\n",
      "Batch 4/165 - Loss: 1.0632\n",
      "Batch 5/165 - Loss: 1.0551\n",
      "Batch 6/165 - Loss: 1.0462\n",
      "Batch 7/165 - Loss: 1.0574\n",
      "Batch 8/165 - Loss: 1.0200\n",
      "Batch 9/165 - Loss: 1.0221\n",
      "Batch 10/165 - Loss: 1.0223\n",
      "Batch 11/165 - Loss: 1.0074\n",
      "Batch 12/165 - Loss: 0.9991\n",
      "Batch 13/165 - Loss: 0.9793\n",
      "Batch 14/165 - Loss: 0.9649\n",
      "Batch 15/165 - Loss: 0.9360\n",
      "Batch 16/165 - Loss: 0.9489\n",
      "Batch 17/165 - Loss: 0.8869\n",
      "Batch 18/165 - Loss: 0.8412\n",
      "Batch 19/165 - Loss: 0.8391\n",
      "Batch 20/165 - Loss: 0.8198\n",
      "Batch 21/165 - Loss: 0.7883\n",
      "Batch 22/165 - Loss: 0.7652\n",
      "Batch 23/165 - Loss: 0.7097\n",
      "Batch 24/165 - Loss: 0.6749\n",
      "Batch 25/165 - Loss: 0.6782\n",
      "Batch 26/165 - Loss: 0.6861\n",
      "Batch 27/165 - Loss: 0.6827\n",
      "Batch 28/165 - Loss: 0.6918\n",
      "Batch 29/165 - Loss: 0.5356\n",
      "Batch 30/165 - Loss: 0.4843\n",
      "Batch 31/165 - Loss: 0.5146\n",
      "Batch 32/165 - Loss: 0.5993\n",
      "Batch 33/165 - Loss: 0.4999\n",
      "Batch 34/165 - Loss: 0.4695\n",
      "Batch 35/165 - Loss: 0.4626\n",
      "Batch 36/165 - Loss: 0.4756\n",
      "Batch 37/165 - Loss: 0.4555\n",
      "Batch 38/165 - Loss: 0.4167\n",
      "Batch 39/165 - Loss: 0.4815\n",
      "Batch 40/165 - Loss: 0.3773\n",
      "Batch 41/165 - Loss: 0.4114\n",
      "Batch 42/165 - Loss: 0.2456\n",
      "Batch 43/165 - Loss: 0.3551\n",
      "Batch 44/165 - Loss: 0.2817\n",
      "Batch 45/165 - Loss: 0.3688\n",
      "Batch 46/165 - Loss: 0.2081\n",
      "Batch 47/165 - Loss: 0.2960\n",
      "Batch 48/165 - Loss: 0.2753\n",
      "Batch 49/165 - Loss: 0.3065\n",
      "Batch 50/165 - Loss: 0.4580\n",
      "Batch 51/165 - Loss: 0.3104\n",
      "Batch 52/165 - Loss: 0.4051\n",
      "Batch 53/165 - Loss: 0.3304\n",
      "Batch 54/165 - Loss: 0.2673\n",
      "Batch 55/165 - Loss: 0.2315\n",
      "Batch 56/165 - Loss: 0.2041\n",
      "Batch 57/165 - Loss: 0.4458\n",
      "Batch 58/165 - Loss: 0.1452\n",
      "Batch 59/165 - Loss: 0.2299\n",
      "Batch 60/165 - Loss: 0.2151\n",
      "Batch 61/165 - Loss: 0.4268\n",
      "Batch 62/165 - Loss: 0.1906\n",
      "Batch 63/165 - Loss: 0.3151\n",
      "Batch 64/165 - Loss: 0.2919\n",
      "Batch 65/165 - Loss: 0.1691\n",
      "Batch 66/165 - Loss: 0.1400\n",
      "Batch 67/165 - Loss: 0.2043\n",
      "Batch 68/165 - Loss: 0.1342\n",
      "Batch 69/165 - Loss: 0.1253\n",
      "Batch 70/165 - Loss: 0.3488\n",
      "Batch 71/165 - Loss: 0.1169\n",
      "Batch 72/165 - Loss: 0.1227\n",
      "Batch 73/165 - Loss: 0.2969\n",
      "Batch 74/165 - Loss: 0.1645\n",
      "Batch 75/165 - Loss: 0.1845\n",
      "Batch 76/165 - Loss: 0.1209\n",
      "Batch 77/165 - Loss: 0.1407\n",
      "Batch 78/165 - Loss: 0.1233\n",
      "Batch 79/165 - Loss: 0.1272\n",
      "Batch 80/165 - Loss: 0.1957\n",
      "Batch 81/165 - Loss: 0.0752\n",
      "Batch 82/165 - Loss: 0.1223\n",
      "Batch 83/165 - Loss: 0.1297\n",
      "Batch 84/165 - Loss: 0.0603\n",
      "Batch 85/165 - Loss: 0.1050\n",
      "Batch 86/165 - Loss: 0.1464\n",
      "Batch 87/165 - Loss: 0.2305\n",
      "Batch 88/165 - Loss: 0.0503\n",
      "Batch 89/165 - Loss: 0.3604\n",
      "Batch 90/165 - Loss: 0.1557\n",
      "Batch 91/165 - Loss: 0.1861\n",
      "Batch 92/165 - Loss: 0.1210\n",
      "Batch 93/165 - Loss: 0.1356\n",
      "Batch 94/165 - Loss: 0.2236\n",
      "Batch 95/165 - Loss: 0.1241\n",
      "Batch 96/165 - Loss: 0.5052\n",
      "Batch 97/165 - Loss: 0.0864\n",
      "Batch 98/165 - Loss: 0.0522\n",
      "Batch 99/165 - Loss: 0.0724\n",
      "Batch 100/165 - Loss: 0.1093\n",
      "Batch 101/165 - Loss: 0.0423\n",
      "Batch 102/165 - Loss: 0.1253\n",
      "Batch 103/165 - Loss: 0.0996\n",
      "Batch 104/165 - Loss: 0.0573\n",
      "Batch 105/165 - Loss: 0.2570\n",
      "Batch 106/165 - Loss: 0.0644\n",
      "Batch 107/165 - Loss: 0.0963\n",
      "Batch 108/165 - Loss: 0.2622\n",
      "Batch 109/165 - Loss: 0.0465\n",
      "Batch 110/165 - Loss: 0.0841\n",
      "Batch 111/165 - Loss: 0.0325\n",
      "Batch 112/165 - Loss: 0.0848\n",
      "Batch 113/165 - Loss: 0.4994\n",
      "Batch 114/165 - Loss: 0.1527\n",
      "Batch 115/165 - Loss: 0.0907\n",
      "Batch 116/165 - Loss: 0.3610\n",
      "Batch 117/165 - Loss: 0.0733\n",
      "Batch 118/165 - Loss: 0.0630\n",
      "Batch 119/165 - Loss: 0.2035\n",
      "Batch 120/165 - Loss: 0.0820\n",
      "Batch 121/165 - Loss: 0.0624\n",
      "Batch 122/165 - Loss: 0.0503\n",
      "Batch 123/165 - Loss: 0.1038\n",
      "Batch 124/165 - Loss: 0.0831\n",
      "Batch 125/165 - Loss: 0.0532\n",
      "Batch 126/165 - Loss: 0.0472\n",
      "Batch 127/165 - Loss: 0.1326\n",
      "Batch 128/165 - Loss: 0.0430\n",
      "Batch 129/165 - Loss: 0.0300\n",
      "Batch 130/165 - Loss: 0.1094\n",
      "Batch 131/165 - Loss: 0.0231\n",
      "Batch 132/165 - Loss: 0.0466\n",
      "Batch 133/165 - Loss: 0.1146\n",
      "Batch 134/165 - Loss: 0.0283\n",
      "Batch 135/165 - Loss: 0.0373\n",
      "Batch 136/165 - Loss: 0.0356\n",
      "Batch 137/165 - Loss: 0.0743\n",
      "Batch 138/165 - Loss: 0.1344\n",
      "Batch 139/165 - Loss: 0.0741\n",
      "Batch 140/165 - Loss: 0.5983\n",
      "Batch 141/165 - Loss: 0.1254\n",
      "Batch 142/165 - Loss: 0.0691\n",
      "Batch 143/165 - Loss: 0.1070\n",
      "Batch 144/165 - Loss: 0.2432\n",
      "Batch 145/165 - Loss: 0.2296\n",
      "Batch 146/165 - Loss: 0.1266\n",
      "Batch 147/165 - Loss: 0.1296\n",
      "Batch 148/165 - Loss: 0.1323\n",
      "Batch 149/165 - Loss: 0.0238\n",
      "Batch 150/165 - Loss: 0.1029\n",
      "Batch 151/165 - Loss: 0.1947\n",
      "Batch 152/165 - Loss: 0.1052\n",
      "Batch 153/165 - Loss: 0.0627\n",
      "Batch 154/165 - Loss: 0.2248\n",
      "Batch 155/165 - Loss: 0.0827\n",
      "Batch 156/165 - Loss: 0.0639\n",
      "Batch 157/165 - Loss: 0.1513\n",
      "Batch 158/165 - Loss: 0.3540\n",
      "Batch 159/165 - Loss: 0.0338\n",
      "Batch 160/165 - Loss: 0.0678\n",
      "Batch 161/165 - Loss: 0.0520\n",
      "Batch 162/165 - Loss: 0.1113\n",
      "Batch 163/165 - Loss: 0.0516\n",
      "Batch 164/165 - Loss: 0.0413\n",
      "Batch 165/165 - Loss: 0.1472\n",
      "Epoch 1/15 | Train Acc: 0.886 | Val Acc: 0.966\n",
      "Batch 1/165 - Loss: 0.1010\n",
      "Batch 2/165 - Loss: 0.0913\n",
      "Batch 3/165 - Loss: 0.1238\n",
      "Batch 4/165 - Loss: 0.0460\n",
      "Batch 5/165 - Loss: 0.0309\n",
      "Batch 6/165 - Loss: 0.0423\n",
      "Batch 7/165 - Loss: 0.7056\n",
      "Batch 8/165 - Loss: 0.0529\n",
      "Batch 9/165 - Loss: 0.0435\n",
      "Batch 10/165 - Loss: 0.0469\n",
      "Batch 11/165 - Loss: 0.0341\n",
      "Batch 12/165 - Loss: 0.0451\n",
      "Batch 13/165 - Loss: 0.0569\n",
      "Batch 14/165 - Loss: 0.0746\n",
      "Batch 15/165 - Loss: 0.0381\n",
      "Batch 16/165 - Loss: 0.0385\n",
      "Batch 17/165 - Loss: 0.0978\n",
      "Batch 18/165 - Loss: 0.0965\n",
      "Batch 19/165 - Loss: 0.0279\n",
      "Batch 20/165 - Loss: 0.5773\n",
      "Batch 21/165 - Loss: 0.0468\n",
      "Batch 22/165 - Loss: 0.0224\n",
      "Batch 23/165 - Loss: 0.1105\n",
      "Batch 24/165 - Loss: 0.0200\n",
      "Batch 25/165 - Loss: 0.0415\n",
      "Batch 26/165 - Loss: 0.0356\n",
      "Batch 27/165 - Loss: 0.0382\n",
      "Batch 28/165 - Loss: 0.0392\n",
      "Batch 29/165 - Loss: 0.0181\n",
      "Batch 30/165 - Loss: 0.0642\n",
      "Batch 31/165 - Loss: 0.0286\n",
      "Batch 32/165 - Loss: 0.0295\n",
      "Batch 33/165 - Loss: 0.0653\n",
      "Batch 34/165 - Loss: 0.4157\n",
      "Batch 35/165 - Loss: 0.0306\n",
      "Batch 36/165 - Loss: 0.0524\n",
      "Batch 37/165 - Loss: 0.1587\n",
      "Batch 38/165 - Loss: 0.0250\n",
      "Batch 39/165 - Loss: 0.1147\n",
      "Batch 40/165 - Loss: 0.0671\n",
      "Batch 41/165 - Loss: 0.0354\n",
      "Batch 42/165 - Loss: 0.0187\n",
      "Batch 43/165 - Loss: 0.0833\n",
      "Batch 44/165 - Loss: 0.0298\n",
      "Batch 45/165 - Loss: 0.1255\n",
      "Batch 46/165 - Loss: 0.0291\n",
      "Batch 47/165 - Loss: 0.0206\n",
      "Batch 48/165 - Loss: 0.0750\n",
      "Batch 49/165 - Loss: 0.0287\n",
      "Batch 50/165 - Loss: 0.1444\n",
      "Batch 51/165 - Loss: 0.0462\n",
      "Batch 52/165 - Loss: 0.1733\n",
      "Batch 53/165 - Loss: 0.0468\n",
      "Batch 54/165 - Loss: 0.0282\n",
      "Batch 55/165 - Loss: 0.0171\n",
      "Batch 56/165 - Loss: 0.0649\n",
      "Batch 57/165 - Loss: 0.2987\n",
      "Batch 58/165 - Loss: 0.0289\n",
      "Batch 59/165 - Loss: 0.0462\n",
      "Batch 60/165 - Loss: 0.0679\n",
      "Batch 61/165 - Loss: 0.2190\n",
      "Batch 62/165 - Loss: 0.0302\n",
      "Batch 63/165 - Loss: 0.2300\n",
      "Batch 64/165 - Loss: 0.2501\n",
      "Batch 65/165 - Loss: 0.0417\n",
      "Batch 66/165 - Loss: 0.0298\n",
      "Batch 67/165 - Loss: 0.1018\n",
      "Batch 68/165 - Loss: 0.0299\n",
      "Batch 69/165 - Loss: 0.0128\n",
      "Batch 70/165 - Loss: 0.0385\n",
      "Batch 71/165 - Loss: 0.0188\n",
      "Batch 72/165 - Loss: 0.0164\n",
      "Batch 73/165 - Loss: 0.0368\n",
      "Batch 74/165 - Loss: 0.0152\n",
      "Batch 75/165 - Loss: 0.0162\n",
      "Batch 76/165 - Loss: 0.0171\n",
      "Batch 77/165 - Loss: 0.0276\n",
      "Batch 78/165 - Loss: 0.0356\n",
      "Batch 79/165 - Loss: 0.0137\n",
      "Batch 80/165 - Loss: 0.1510\n",
      "Batch 81/165 - Loss: 0.0199\n",
      "Batch 82/165 - Loss: 0.0144\n",
      "Batch 83/165 - Loss: 0.0726\n",
      "Batch 84/165 - Loss: 0.0140\n",
      "Batch 85/165 - Loss: 0.0177\n",
      "Batch 86/165 - Loss: 0.0147\n",
      "Batch 87/165 - Loss: 0.1497\n",
      "Batch 88/165 - Loss: 0.0165\n",
      "Batch 89/165 - Loss: 0.0895\n",
      "Batch 90/165 - Loss: 0.0669\n",
      "Batch 91/165 - Loss: 0.0644\n",
      "Batch 92/165 - Loss: 0.0156\n",
      "Batch 93/165 - Loss: 0.0099\n",
      "Batch 94/165 - Loss: 0.1562\n",
      "Batch 95/165 - Loss: 0.0174\n",
      "Batch 96/165 - Loss: 0.3598\n",
      "Batch 97/165 - Loss: 0.0248\n",
      "Batch 98/165 - Loss: 0.0082\n",
      "Batch 99/165 - Loss: 0.0639\n",
      "Batch 100/165 - Loss: 0.0311\n",
      "Batch 101/165 - Loss: 0.0078\n",
      "Batch 102/165 - Loss: 0.0088\n",
      "Batch 103/165 - Loss: 0.0217\n",
      "Batch 104/165 - Loss: 0.0260\n",
      "Batch 105/165 - Loss: 0.1604\n",
      "Batch 106/165 - Loss: 0.0141\n",
      "Batch 107/165 - Loss: 0.0202\n",
      "Batch 108/165 - Loss: 0.0638\n",
      "Batch 109/165 - Loss: 0.0271\n",
      "Batch 110/165 - Loss: 0.0129\n",
      "Batch 111/165 - Loss: 0.0090\n",
      "Batch 112/165 - Loss: 0.0408\n",
      "Batch 113/165 - Loss: 0.2793\n",
      "Batch 114/165 - Loss: 0.0624\n",
      "Batch 115/165 - Loss: 0.0199\n",
      "Batch 116/165 - Loss: 0.2323\n",
      "Batch 117/165 - Loss: 0.0293\n",
      "Batch 118/165 - Loss: 0.0621\n",
      "Batch 119/165 - Loss: 0.2110\n",
      "Batch 120/165 - Loss: 0.1367\n",
      "Batch 121/165 - Loss: 0.0166\n",
      "Batch 122/165 - Loss: 0.0305\n",
      "Batch 123/165 - Loss: 0.0184\n",
      "Batch 124/165 - Loss: 0.0248\n",
      "Batch 125/165 - Loss: 0.0207\n",
      "Batch 126/165 - Loss: 0.0468\n",
      "Batch 127/165 - Loss: 0.0614\n",
      "Batch 128/165 - Loss: 0.0110\n",
      "Batch 129/165 - Loss: 0.0078\n",
      "Batch 130/165 - Loss: 0.0371\n",
      "Batch 131/165 - Loss: 0.0426\n",
      "Batch 132/165 - Loss: 0.0125\n",
      "Batch 133/165 - Loss: 0.1384\n",
      "Batch 134/165 - Loss: 0.0127\n",
      "Batch 135/165 - Loss: 0.0132\n",
      "Batch 136/165 - Loss: 0.0395\n",
      "Batch 137/165 - Loss: 0.0998\n",
      "Batch 138/165 - Loss: 0.1526\n",
      "Batch 139/165 - Loss: 0.0085\n",
      "Batch 140/165 - Loss: 0.2041\n",
      "Batch 141/165 - Loss: 0.0215\n",
      "Batch 142/165 - Loss: 0.0134\n",
      "Batch 143/165 - Loss: 0.0284\n",
      "Batch 144/165 - Loss: 0.0867\n",
      "Batch 145/165 - Loss: 0.0449\n",
      "Batch 146/165 - Loss: 0.1166\n",
      "Batch 147/165 - Loss: 0.0243\n",
      "Batch 148/165 - Loss: 0.0677\n",
      "Batch 149/165 - Loss: 0.0161\n",
      "Batch 150/165 - Loss: 0.1022\n",
      "Batch 151/165 - Loss: 0.0648\n",
      "Batch 152/165 - Loss: 0.0433\n",
      "Batch 153/165 - Loss: 0.0365\n",
      "Batch 154/165 - Loss: 0.0926\n",
      "Batch 155/165 - Loss: 0.0251\n",
      "Batch 156/165 - Loss: 0.0279\n",
      "Batch 157/165 - Loss: 0.0196\n",
      "Batch 158/165 - Loss: 0.1737\n",
      "Batch 159/165 - Loss: 0.0092\n",
      "Batch 160/165 - Loss: 0.0327\n",
      "Batch 161/165 - Loss: 0.0176\n",
      "Batch 162/165 - Loss: 0.0278\n",
      "Batch 163/165 - Loss: 0.0164\n",
      "Batch 164/165 - Loss: 0.0262\n",
      "Batch 165/165 - Loss: 0.0393\n",
      "Epoch 2/15 | Train Acc: 0.976 | Val Acc: 0.997\n",
      "Batch 1/165 - Loss: 0.0240\n",
      "Batch 2/165 - Loss: 0.0069\n",
      "Batch 3/165 - Loss: 0.0129\n",
      "Batch 4/165 - Loss: 0.0178\n",
      "Batch 5/165 - Loss: 0.0224\n",
      "Batch 6/165 - Loss: 0.0723\n",
      "Batch 7/165 - Loss: 0.4446\n",
      "Batch 8/165 - Loss: 0.0088\n",
      "Batch 9/165 - Loss: 0.0063\n",
      "Batch 10/165 - Loss: 0.0075\n",
      "Batch 11/165 - Loss: 0.0077\n",
      "Batch 12/165 - Loss: 0.0446\n",
      "Batch 13/165 - Loss: 0.0069\n",
      "Batch 14/165 - Loss: 0.1629\n",
      "Batch 15/165 - Loss: 0.0081\n",
      "Batch 16/165 - Loss: 0.0099\n",
      "Batch 17/165 - Loss: 0.0471\n",
      "Batch 18/165 - Loss: 0.0148\n",
      "Batch 19/165 - Loss: 0.0135\n",
      "Batch 20/165 - Loss: 0.4454\n",
      "Batch 21/165 - Loss: 0.0128\n",
      "Batch 22/165 - Loss: 0.0113\n",
      "Batch 23/165 - Loss: 0.0756\n",
      "Batch 24/165 - Loss: 0.0068\n",
      "Batch 25/165 - Loss: 0.0131\n",
      "Batch 26/165 - Loss: 0.0070\n",
      "Batch 27/165 - Loss: 0.0124\n",
      "Batch 28/165 - Loss: 0.0157\n",
      "Batch 29/165 - Loss: 0.0092\n",
      "Batch 30/165 - Loss: 0.0520\n",
      "Batch 31/165 - Loss: 0.0106\n",
      "Batch 32/165 - Loss: 0.0071\n",
      "Batch 33/165 - Loss: 0.0201\n",
      "Batch 34/165 - Loss: 0.2736\n",
      "Batch 35/165 - Loss: 0.0242\n",
      "Batch 36/165 - Loss: 0.0211\n",
      "Batch 37/165 - Loss: 0.0902\n",
      "Batch 38/165 - Loss: 0.0098\n",
      "Batch 39/165 - Loss: 0.0905\n",
      "Batch 40/165 - Loss: 0.0489\n",
      "Batch 41/165 - Loss: 0.0318\n",
      "Batch 42/165 - Loss: 0.0124\n",
      "Batch 43/165 - Loss: 0.0581\n",
      "Batch 44/165 - Loss: 0.0244\n",
      "Batch 45/165 - Loss: 0.1172\n",
      "Batch 46/165 - Loss: 0.0218\n",
      "Batch 47/165 - Loss: 0.0154\n",
      "Batch 48/165 - Loss: 0.0432\n",
      "Batch 49/165 - Loss: 0.0231\n",
      "Batch 50/165 - Loss: 0.0797\n",
      "Batch 51/165 - Loss: 0.0215\n",
      "Batch 52/165 - Loss: 0.1068\n",
      "Batch 53/165 - Loss: 0.0187\n",
      "Batch 54/165 - Loss: 0.0134\n",
      "Batch 55/165 - Loss: 0.0125\n",
      "Batch 56/165 - Loss: 0.0780\n",
      "Batch 57/165 - Loss: 0.1518\n",
      "Batch 58/165 - Loss: 0.0263\n",
      "Batch 59/165 - Loss: 0.0137\n",
      "Batch 60/165 - Loss: 0.0177\n",
      "Batch 61/165 - Loss: 0.1207\n",
      "Batch 62/165 - Loss: 0.0090\n",
      "Batch 63/165 - Loss: 0.0990\n",
      "Batch 64/165 - Loss: 0.1096\n",
      "Batch 65/165 - Loss: 0.0069\n",
      "Batch 66/165 - Loss: 0.0383\n",
      "Batch 67/165 - Loss: 0.0284\n",
      "Batch 68/165 - Loss: 0.0141\n",
      "Batch 69/165 - Loss: 0.0043\n",
      "Batch 70/165 - Loss: 0.0121\n",
      "Batch 71/165 - Loss: 0.0228\n",
      "Batch 72/165 - Loss: 0.0090\n",
      "Batch 73/165 - Loss: 0.0349\n",
      "Batch 74/165 - Loss: 0.0119\n",
      "Batch 75/165 - Loss: 0.0131\n",
      "Batch 76/165 - Loss: 0.0147\n",
      "Batch 77/165 - Loss: 0.0085\n",
      "Batch 78/165 - Loss: 0.0134\n",
      "Batch 79/165 - Loss: 0.0059\n",
      "Batch 80/165 - Loss: 0.1481\n",
      "Batch 81/165 - Loss: 0.0052\n",
      "Batch 82/165 - Loss: 0.0097\n",
      "Batch 83/165 - Loss: 0.0296\n",
      "Batch 84/165 - Loss: 0.0104\n",
      "Batch 85/165 - Loss: 0.0300\n",
      "Batch 86/165 - Loss: 0.0070\n",
      "Batch 87/165 - Loss: 0.1040\n",
      "Batch 88/165 - Loss: 0.0045\n",
      "Batch 89/165 - Loss: 0.1521\n",
      "Batch 90/165 - Loss: 0.0631\n",
      "Batch 91/165 - Loss: 0.0569\n",
      "Batch 92/165 - Loss: 0.0234\n",
      "Batch 93/165 - Loss: 0.0044\n",
      "Batch 94/165 - Loss: 0.0692\n",
      "Batch 95/165 - Loss: 0.0131\n",
      "Batch 96/165 - Loss: 0.2674\n",
      "Batch 97/165 - Loss: 0.0201\n",
      "Batch 98/165 - Loss: 0.0051\n",
      "Batch 99/165 - Loss: 0.0535\n",
      "Batch 100/165 - Loss: 0.0166\n",
      "Batch 101/165 - Loss: 0.0073\n",
      "Batch 102/165 - Loss: 0.0073\n",
      "Batch 103/165 - Loss: 0.0963\n",
      "Batch 104/165 - Loss: 0.0151\n",
      "Batch 105/165 - Loss: 0.1222\n",
      "Batch 106/165 - Loss: 0.0109\n",
      "Batch 107/165 - Loss: 0.0358\n",
      "Batch 108/165 - Loss: 0.0267\n",
      "Batch 109/165 - Loss: 0.0306\n",
      "Batch 110/165 - Loss: 0.0110\n",
      "Batch 111/165 - Loss: 0.0056\n",
      "Batch 112/165 - Loss: 0.0674\n",
      "Batch 113/165 - Loss: 0.2434\n",
      "Batch 114/165 - Loss: 0.0776\n",
      "Batch 115/165 - Loss: 0.0285\n",
      "Batch 116/165 - Loss: 0.2097\n",
      "Batch 117/165 - Loss: 0.0213\n",
      "Batch 118/165 - Loss: 0.0373\n",
      "Batch 119/165 - Loss: 0.1856\n",
      "Batch 120/165 - Loss: 0.2221\n",
      "Batch 121/165 - Loss: 0.0140\n",
      "Batch 122/165 - Loss: 0.0547\n",
      "Batch 123/165 - Loss: 0.0116\n",
      "Batch 124/165 - Loss: 0.0117\n",
      "Batch 125/165 - Loss: 0.0168\n",
      "Batch 126/165 - Loss: 0.0281\n",
      "Batch 127/165 - Loss: 0.0415\n",
      "Batch 128/165 - Loss: 0.0076\n",
      "Batch 129/165 - Loss: 0.0079\n",
      "Batch 130/165 - Loss: 0.0194\n",
      "Batch 131/165 - Loss: 0.0262\n",
      "Batch 132/165 - Loss: 0.0099\n",
      "Batch 133/165 - Loss: 0.1091\n",
      "Batch 134/165 - Loss: 0.0135\n",
      "Batch 135/165 - Loss: 0.0061\n",
      "Batch 136/165 - Loss: 0.0353\n",
      "Batch 137/165 - Loss: 0.0944\n",
      "Batch 138/165 - Loss: 0.0976\n",
      "Batch 139/165 - Loss: 0.0072\n",
      "Batch 140/165 - Loss: 0.0936\n",
      "Batch 141/165 - Loss: 0.0191\n",
      "Batch 142/165 - Loss: 0.0077\n",
      "Batch 143/165 - Loss: 0.0391\n",
      "Batch 144/165 - Loss: 0.0243\n",
      "Batch 145/165 - Loss: 0.0476\n",
      "Batch 146/165 - Loss: 0.0795\n",
      "Batch 147/165 - Loss: 0.0181\n",
      "Batch 148/165 - Loss: 0.0541\n",
      "Batch 149/165 - Loss: 0.0129\n",
      "Batch 150/165 - Loss: 0.1128\n",
      "Batch 151/165 - Loss: 0.0650\n",
      "Batch 152/165 - Loss: 0.0334\n",
      "Batch 153/165 - Loss: 0.0266\n",
      "Batch 154/165 - Loss: 0.0867\n",
      "Batch 155/165 - Loss: 0.0107\n",
      "Batch 156/165 - Loss: 0.0207\n",
      "Batch 157/165 - Loss: 0.0160\n",
      "Batch 158/165 - Loss: 0.1678\n",
      "Batch 159/165 - Loss: 0.0051\n",
      "Batch 160/165 - Loss: 0.0262\n",
      "Batch 161/165 - Loss: 0.0138\n",
      "Batch 162/165 - Loss: 0.0232\n",
      "Batch 163/165 - Loss: 0.0112\n",
      "Batch 164/165 - Loss: 0.0086\n",
      "Batch 165/165 - Loss: 0.0236\n",
      "Epoch 3/15 | Train Acc: 0.984 | Val Acc: 0.997\n",
      "Batch 1/165 - Loss: 0.0153\n",
      "Batch 2/165 - Loss: 0.0041\n",
      "Batch 3/165 - Loss: 0.0084\n",
      "Batch 4/165 - Loss: 0.0153\n",
      "Batch 5/165 - Loss: 0.0359\n",
      "Batch 6/165 - Loss: 0.0876\n",
      "Batch 7/165 - Loss: 0.2964\n",
      "Batch 8/165 - Loss: 0.0082\n",
      "Batch 9/165 - Loss: 0.0098\n",
      "Batch 10/165 - Loss: 0.0037\n",
      "Batch 11/165 - Loss: 0.0046\n",
      "Batch 12/165 - Loss: 0.0174\n",
      "Batch 13/165 - Loss: 0.0039\n",
      "Batch 14/165 - Loss: 0.1115\n",
      "Batch 15/165 - Loss: 0.0047\n",
      "Batch 16/165 - Loss: 0.0044\n",
      "Batch 17/165 - Loss: 0.0308\n",
      "Batch 18/165 - Loss: 0.0074\n",
      "Batch 19/165 - Loss: 0.0052\n",
      "Batch 20/165 - Loss: 0.3098\n",
      "Batch 21/165 - Loss: 0.0047\n",
      "Batch 22/165 - Loss: 0.0062\n",
      "Batch 23/165 - Loss: 0.0593\n",
      "Batch 24/165 - Loss: 0.0037\n",
      "Batch 25/165 - Loss: 0.0054\n",
      "Batch 26/165 - Loss: 0.0046\n",
      "Batch 27/165 - Loss: 0.0107\n",
      "Batch 28/165 - Loss: 0.0080\n",
      "Batch 29/165 - Loss: 0.0040\n",
      "Batch 30/165 - Loss: 0.0275\n",
      "Batch 31/165 - Loss: 0.0056\n",
      "Batch 32/165 - Loss: 0.0033\n",
      "Batch 33/165 - Loss: 0.0147\n",
      "Batch 34/165 - Loss: 0.2491\n",
      "Batch 35/165 - Loss: 0.0205\n",
      "Batch 36/165 - Loss: 0.0108\n",
      "Batch 37/165 - Loss: 0.1205\n",
      "Batch 38/165 - Loss: 0.0048\n",
      "Batch 39/165 - Loss: 0.0711\n",
      "Batch 40/165 - Loss: 0.0283\n",
      "Batch 41/165 - Loss: 0.0308\n",
      "Batch 42/165 - Loss: 0.0074\n",
      "Batch 43/165 - Loss: 0.0389\n",
      "Batch 44/165 - Loss: 0.0210\n",
      "Batch 45/165 - Loss: 0.1112\n",
      "Batch 46/165 - Loss: 0.0146\n",
      "Batch 47/165 - Loss: 0.0107\n",
      "Batch 48/165 - Loss: 0.0346\n",
      "Batch 49/165 - Loss: 0.0184\n",
      "Batch 50/165 - Loss: 0.0921\n",
      "Batch 51/165 - Loss: 0.0171\n",
      "Batch 52/165 - Loss: 0.1327\n",
      "Batch 53/165 - Loss: 0.0189\n",
      "Batch 54/165 - Loss: 0.0254\n",
      "Batch 55/165 - Loss: 0.0173\n",
      "Batch 56/165 - Loss: 0.0736\n",
      "Batch 57/165 - Loss: 0.1090\n",
      "Batch 58/165 - Loss: 0.0396\n",
      "Batch 59/165 - Loss: 0.0110\n",
      "Batch 60/165 - Loss: 0.0175\n",
      "Batch 61/165 - Loss: 0.1166\n",
      "Batch 62/165 - Loss: 0.0074\n",
      "Batch 63/165 - Loss: 0.0581\n",
      "Batch 64/165 - Loss: 0.0875\n",
      "Batch 65/165 - Loss: 0.0053\n",
      "Batch 66/165 - Loss: 0.0408\n",
      "Batch 67/165 - Loss: 0.0311\n",
      "Batch 68/165 - Loss: 0.0138\n",
      "Batch 69/165 - Loss: 0.0033\n",
      "Batch 70/165 - Loss: 0.0074\n",
      "Batch 71/165 - Loss: 0.0599\n",
      "Batch 72/165 - Loss: 0.0092\n",
      "Batch 73/165 - Loss: 0.0174\n",
      "Batch 74/165 - Loss: 0.0161\n",
      "Batch 75/165 - Loss: 0.0086\n",
      "Batch 76/165 - Loss: 0.0088\n",
      "Batch 77/165 - Loss: 0.0040\n",
      "Batch 78/165 - Loss: 0.0073\n",
      "Batch 79/165 - Loss: 0.0053\n",
      "Batch 80/165 - Loss: 0.1515\n",
      "Batch 81/165 - Loss: 0.0039\n",
      "Batch 82/165 - Loss: 0.0124\n",
      "Batch 83/165 - Loss: 0.0433\n",
      "Batch 84/165 - Loss: 0.0101\n",
      "Batch 85/165 - Loss: 0.0192\n",
      "Batch 86/165 - Loss: 0.0094\n",
      "Batch 87/165 - Loss: 0.0614\n",
      "Batch 88/165 - Loss: 0.0038\n",
      "Batch 89/165 - Loss: 0.1383\n",
      "Batch 90/165 - Loss: 0.0602\n",
      "Batch 91/165 - Loss: 0.0551\n",
      "Batch 92/165 - Loss: 0.0538\n",
      "Batch 93/165 - Loss: 0.0028\n",
      "Batch 94/165 - Loss: 0.1414\n",
      "Batch 95/165 - Loss: 0.0098\n",
      "Batch 96/165 - Loss: 0.3666\n",
      "Batch 97/165 - Loss: 0.0121\n",
      "Batch 98/165 - Loss: 0.0052\n",
      "Batch 99/165 - Loss: 0.0455\n",
      "Batch 100/165 - Loss: 0.0140\n",
      "Batch 101/165 - Loss: 0.0057\n",
      "Batch 102/165 - Loss: 0.0060\n",
      "Batch 103/165 - Loss: 0.0963\n",
      "Batch 104/165 - Loss: 0.0528\n",
      "Batch 105/165 - Loss: 0.1438\n",
      "Batch 106/165 - Loss: 0.0298\n",
      "Batch 107/165 - Loss: 0.0823\n",
      "Batch 108/165 - Loss: 0.0500\n",
      "Batch 109/165 - Loss: 0.0678\n",
      "Batch 110/165 - Loss: 0.0278\n",
      "Batch 111/165 - Loss: 0.0074\n",
      "Batch 112/165 - Loss: 0.0459\n",
      "Batch 113/165 - Loss: 0.2245\n",
      "Batch 114/165 - Loss: 0.0795\n",
      "Batch 115/165 - Loss: 0.0394\n",
      "Batch 116/165 - Loss: 0.2438\n",
      "Batch 117/165 - Loss: 0.0169\n",
      "Batch 118/165 - Loss: 0.0247\n",
      "Batch 119/165 - Loss: 0.0840\n",
      "Batch 120/165 - Loss: 0.0827\n",
      "Batch 121/165 - Loss: 0.0133\n",
      "Batch 122/165 - Loss: 0.0541\n",
      "Batch 123/165 - Loss: 0.0134\n",
      "Batch 124/165 - Loss: 0.0145\n",
      "Batch 125/165 - Loss: 0.0271\n",
      "Batch 126/165 - Loss: 0.0200\n",
      "Batch 127/165 - Loss: 0.0705\n",
      "Batch 128/165 - Loss: 0.0089\n",
      "Batch 129/165 - Loss: 0.0086\n",
      "Batch 130/165 - Loss: 0.0486\n",
      "Batch 131/165 - Loss: 0.0159\n",
      "Batch 132/165 - Loss: 0.0187\n",
      "Batch 133/165 - Loss: 0.0606\n",
      "Batch 134/165 - Loss: 0.0196\n",
      "Batch 135/165 - Loss: 0.0054\n",
      "Batch 136/165 - Loss: 0.0196\n",
      "Batch 137/165 - Loss: 0.1024\n",
      "Batch 138/165 - Loss: 0.0963\n",
      "Batch 139/165 - Loss: 0.0065\n",
      "Batch 140/165 - Loss: 0.1150\n",
      "Batch 141/165 - Loss: 0.0099\n",
      "Batch 142/165 - Loss: 0.0042\n",
      "Batch 143/165 - Loss: 0.0272\n",
      "Batch 144/165 - Loss: 0.0071\n",
      "Batch 145/165 - Loss: 0.0451\n",
      "Batch 146/165 - Loss: 0.0223\n",
      "Batch 147/165 - Loss: 0.0150\n",
      "Batch 148/165 - Loss: 0.0617\n",
      "Batch 149/165 - Loss: 0.0079\n",
      "Batch 150/165 - Loss: 0.0942\n",
      "Batch 151/165 - Loss: 0.0668\n",
      "Batch 152/165 - Loss: 0.0349\n",
      "Batch 153/165 - Loss: 0.0179\n",
      "Batch 154/165 - Loss: 0.0914\n",
      "Batch 155/165 - Loss: 0.0070\n",
      "Batch 156/165 - Loss: 0.0153\n",
      "Batch 157/165 - Loss: 0.0137\n",
      "Batch 158/165 - Loss: 0.0767\n",
      "Batch 159/165 - Loss: 0.0038\n",
      "Batch 160/165 - Loss: 0.0218\n",
      "Batch 161/165 - Loss: 0.0069\n",
      "Batch 162/165 - Loss: 0.0181\n",
      "Batch 163/165 - Loss: 0.0093\n",
      "Batch 164/165 - Loss: 0.0093\n",
      "Batch 165/165 - Loss: 0.0175\n",
      "Epoch 4/15 | Train Acc: 0.984 | Val Acc: 0.998\n",
      "Batch 1/165 - Loss: 0.0137\n",
      "Batch 2/165 - Loss: 0.0026\n",
      "Batch 3/165 - Loss: 0.0067\n",
      "Batch 4/165 - Loss: 0.0174\n",
      "Batch 5/165 - Loss: 0.0314\n",
      "Batch 6/165 - Loss: 0.1526\n",
      "Batch 7/165 - Loss: 0.2354\n",
      "Batch 8/165 - Loss: 0.0058\n",
      "Batch 9/165 - Loss: 0.0057\n",
      "Batch 10/165 - Loss: 0.0024\n",
      "Batch 11/165 - Loss: 0.0033\n",
      "Batch 12/165 - Loss: 0.0380\n",
      "Batch 13/165 - Loss: 0.0057\n",
      "Batch 14/165 - Loss: 0.1728\n",
      "Batch 15/165 - Loss: 0.0030\n",
      "Batch 16/165 - Loss: 0.0066\n",
      "Batch 17/165 - Loss: 0.0336\n",
      "Batch 18/165 - Loss: 0.0052\n",
      "Batch 19/165 - Loss: 0.0062\n",
      "Batch 20/165 - Loss: 0.3008\n",
      "Batch 21/165 - Loss: 0.0039\n",
      "Batch 22/165 - Loss: 0.0150\n",
      "Batch 23/165 - Loss: 0.0500\n",
      "Batch 24/165 - Loss: 0.0024\n",
      "Batch 25/165 - Loss: 0.0039\n",
      "Batch 26/165 - Loss: 0.0028\n",
      "Batch 27/165 - Loss: 0.0072\n",
      "Batch 28/165 - Loss: 0.0082\n",
      "Batch 29/165 - Loss: 0.0024\n",
      "Batch 30/165 - Loss: 0.0223\n",
      "Batch 31/165 - Loss: 0.0041\n",
      "Batch 32/165 - Loss: 0.0018\n",
      "Batch 33/165 - Loss: 0.0185\n",
      "Batch 34/165 - Loss: 0.2480\n",
      "Batch 35/165 - Loss: 0.0182\n",
      "Batch 36/165 - Loss: 0.0075\n",
      "Batch 37/165 - Loss: 0.0944\n",
      "Batch 38/165 - Loss: 0.0032\n",
      "Batch 39/165 - Loss: 0.0727\n",
      "Batch 40/165 - Loss: 0.0197\n",
      "Batch 41/165 - Loss: 0.0264\n",
      "Batch 42/165 - Loss: 0.0061\n",
      "Batch 43/165 - Loss: 0.0371\n",
      "Batch 44/165 - Loss: 0.0209\n",
      "Batch 45/165 - Loss: 0.1074\n",
      "Batch 46/165 - Loss: 0.0083\n",
      "Batch 47/165 - Loss: 0.0086\n",
      "Batch 48/165 - Loss: 0.0349\n",
      "Batch 49/165 - Loss: 0.0128\n",
      "Batch 50/165 - Loss: 0.0682\n",
      "Batch 51/165 - Loss: 0.0124\n",
      "Batch 52/165 - Loss: 0.1127\n",
      "Batch 53/165 - Loss: 0.0145\n",
      "Batch 54/165 - Loss: 0.0209\n",
      "Batch 55/165 - Loss: 0.0190\n",
      "Batch 56/165 - Loss: 0.0726\n",
      "Batch 57/165 - Loss: 0.0963\n",
      "Batch 58/165 - Loss: 0.0493\n",
      "Batch 59/165 - Loss: 0.0103\n",
      "Batch 60/165 - Loss: 0.0184\n",
      "Batch 61/165 - Loss: 0.1299\n",
      "Batch 62/165 - Loss: 0.0073\n",
      "Batch 63/165 - Loss: 0.0548\n",
      "Batch 64/165 - Loss: 0.0616\n",
      "Batch 65/165 - Loss: 0.0034\n",
      "Batch 66/165 - Loss: 0.0417\n",
      "Batch 67/165 - Loss: 0.0274\n",
      "Batch 68/165 - Loss: 0.0116\n",
      "Batch 69/165 - Loss: 0.0025\n",
      "Batch 70/165 - Loss: 0.0066\n",
      "Batch 71/165 - Loss: 0.0529\n",
      "Batch 72/165 - Loss: 0.0104\n",
      "Batch 73/165 - Loss: 0.0154\n",
      "Batch 74/165 - Loss: 0.0163\n",
      "Batch 75/165 - Loss: 0.0085\n",
      "Batch 76/165 - Loss: 0.0087\n",
      "Batch 77/165 - Loss: 0.0033\n",
      "Batch 78/165 - Loss: 0.0049\n",
      "Batch 79/165 - Loss: 0.0051\n",
      "Batch 80/165 - Loss: 0.1350\n",
      "Batch 81/165 - Loss: 0.0027\n",
      "Batch 82/165 - Loss: 0.0135\n",
      "Batch 83/165 - Loss: 0.0411\n",
      "Batch 84/165 - Loss: 0.0102\n",
      "Batch 85/165 - Loss: 0.0160\n",
      "Batch 86/165 - Loss: 0.0117\n",
      "Batch 87/165 - Loss: 0.0404\n",
      "Batch 88/165 - Loss: 0.0031\n",
      "Batch 89/165 - Loss: 0.1386\n",
      "Batch 90/165 - Loss: 0.0517\n",
      "Batch 91/165 - Loss: 0.0356\n",
      "Batch 92/165 - Loss: 0.0214\n",
      "Batch 93/165 - Loss: 0.0024\n",
      "Batch 94/165 - Loss: 0.1532\n",
      "Batch 95/165 - Loss: 0.0125\n",
      "Batch 96/165 - Loss: 0.3282\n",
      "Batch 97/165 - Loss: 0.0080\n",
      "Batch 98/165 - Loss: 0.0091\n",
      "Batch 99/165 - Loss: 0.0325\n",
      "Batch 100/165 - Loss: 0.0072\n",
      "Batch 101/165 - Loss: 0.0027\n",
      "Batch 102/165 - Loss: 0.0037\n",
      "Batch 103/165 - Loss: 0.0653\n",
      "Batch 104/165 - Loss: 0.0148\n",
      "Batch 105/165 - Loss: 0.1024\n",
      "Batch 106/165 - Loss: 0.0079\n",
      "Batch 107/165 - Loss: 0.0278\n",
      "Batch 108/165 - Loss: 0.0405\n",
      "Batch 109/165 - Loss: 0.0413\n",
      "Batch 110/165 - Loss: 0.0109\n",
      "Batch 111/165 - Loss: 0.0025\n",
      "Batch 112/165 - Loss: 0.0587\n",
      "Batch 113/165 - Loss: 0.2614\n",
      "Batch 114/165 - Loss: 0.0881\n",
      "Batch 115/165 - Loss: 0.0485\n",
      "Batch 116/165 - Loss: 0.2935\n",
      "Batch 117/165 - Loss: 0.0071\n",
      "Batch 118/165 - Loss: 0.0111\n",
      "Batch 119/165 - Loss: 0.0373\n",
      "Batch 120/165 - Loss: 0.0873\n",
      "Batch 121/165 - Loss: 0.0097\n",
      "Batch 122/165 - Loss: 0.0653\n",
      "Batch 123/165 - Loss: 0.0117\n",
      "Batch 124/165 - Loss: 0.0104\n",
      "Batch 125/165 - Loss: 0.0361\n",
      "Batch 126/165 - Loss: 0.0134\n",
      "Batch 127/165 - Loss: 0.0898\n",
      "Batch 128/165 - Loss: 0.0121\n",
      "Batch 129/165 - Loss: 0.0086\n",
      "Batch 130/165 - Loss: 0.0771\n",
      "Batch 131/165 - Loss: 0.0086\n",
      "Batch 132/165 - Loss: 0.0183\n",
      "Batch 133/165 - Loss: 0.0376\n",
      "Batch 134/165 - Loss: 0.0164\n",
      "Batch 135/165 - Loss: 0.0030\n",
      "Batch 136/165 - Loss: 0.0131\n",
      "Batch 137/165 - Loss: 0.0798\n",
      "Batch 138/165 - Loss: 0.0971\n",
      "Batch 139/165 - Loss: 0.0036\n",
      "Batch 140/165 - Loss: 0.0853\n",
      "Batch 141/165 - Loss: 0.0063\n",
      "Batch 142/165 - Loss: 0.0030\n",
      "Batch 143/165 - Loss: 0.0359\n",
      "Batch 144/165 - Loss: 0.0055\n",
      "Batch 145/165 - Loss: 0.0550\n",
      "Batch 146/165 - Loss: 0.0190\n",
      "Batch 147/165 - Loss: 0.0162\n",
      "Batch 148/165 - Loss: 0.0701\n",
      "Batch 149/165 - Loss: 0.0056\n",
      "Batch 150/165 - Loss: 0.1036\n",
      "Batch 151/165 - Loss: 0.0530\n",
      "Batch 152/165 - Loss: 0.0269\n",
      "Batch 153/165 - Loss: 0.0164\n",
      "Batch 154/165 - Loss: 0.0909\n",
      "Batch 155/165 - Loss: 0.0059\n",
      "Batch 156/165 - Loss: 0.0142\n",
      "Batch 157/165 - Loss: 0.0129\n",
      "Batch 158/165 - Loss: 0.0793\n",
      "Batch 159/165 - Loss: 0.0045\n",
      "Batch 160/165 - Loss: 0.0191\n",
      "Batch 161/165 - Loss: 0.0085\n",
      "Batch 162/165 - Loss: 0.0186\n",
      "Batch 163/165 - Loss: 0.0112\n",
      "Batch 164/165 - Loss: 0.0074\n",
      "Batch 165/165 - Loss: 0.0147\n",
      "Epoch 5/15 | Train Acc: 0.985 | Val Acc: 0.998\n",
      "Batch 1/165 - Loss: 0.0138\n",
      "Batch 2/165 - Loss: 0.0020\n",
      "Batch 3/165 - Loss: 0.0066\n",
      "Batch 4/165 - Loss: 0.0171\n",
      "Batch 5/165 - Loss: 0.0328\n",
      "Batch 6/165 - Loss: 0.1452\n",
      "Batch 7/165 - Loss: 0.2211\n",
      "Batch 8/165 - Loss: 0.0058\n",
      "Batch 9/165 - Loss: 0.0074\n",
      "Batch 10/165 - Loss: 0.0016\n",
      "Batch 11/165 - Loss: 0.0025\n",
      "Batch 12/165 - Loss: 0.0257\n",
      "Batch 13/165 - Loss: 0.0039\n",
      "Batch 14/165 - Loss: 0.1294\n",
      "Batch 15/165 - Loss: 0.0024\n",
      "Batch 16/165 - Loss: 0.0046\n",
      "Batch 17/165 - Loss: 0.0305\n",
      "Batch 18/165 - Loss: 0.0052\n",
      "Batch 19/165 - Loss: 0.0056\n",
      "Batch 20/165 - Loss: 0.2838\n",
      "Batch 21/165 - Loss: 0.0037\n",
      "Batch 22/165 - Loss: 0.0240\n",
      "Batch 23/165 - Loss: 0.0424\n",
      "Batch 24/165 - Loss: 0.0020\n",
      "Batch 25/165 - Loss: 0.0042\n",
      "Batch 26/165 - Loss: 0.0022\n",
      "Batch 27/165 - Loss: 0.0086\n",
      "Batch 28/165 - Loss: 0.0072\n",
      "Batch 29/165 - Loss: 0.0020\n",
      "Batch 30/165 - Loss: 0.0205\n",
      "Batch 31/165 - Loss: 0.0030\n",
      "Batch 32/165 - Loss: 0.0014\n",
      "Batch 33/165 - Loss: 0.0182\n",
      "Batch 34/165 - Loss: 0.2205\n",
      "Batch 35/165 - Loss: 0.0196\n",
      "Batch 36/165 - Loss: 0.0078\n",
      "Batch 37/165 - Loss: 0.0936\n",
      "Batch 38/165 - Loss: 0.0027\n",
      "Batch 39/165 - Loss: 0.0640\n",
      "Batch 40/165 - Loss: 0.0170\n",
      "Batch 41/165 - Loss: 0.0306\n",
      "Batch 42/165 - Loss: 0.0054\n",
      "Batch 43/165 - Loss: 0.0321\n",
      "Batch 44/165 - Loss: 0.0237\n",
      "Batch 45/165 - Loss: 0.1010\n",
      "Batch 46/165 - Loss: 0.0073\n",
      "Batch 47/165 - Loss: 0.0085\n",
      "Batch 48/165 - Loss: 0.0348\n",
      "Batch 49/165 - Loss: 0.0132\n",
      "Batch 50/165 - Loss: 0.0625\n",
      "Batch 51/165 - Loss: 0.0107\n",
      "Batch 52/165 - Loss: 0.1374\n",
      "Batch 53/165 - Loss: 0.0165\n",
      "Batch 54/165 - Loss: 0.0102\n",
      "Batch 55/165 - Loss: 0.0202\n",
      "Batch 56/165 - Loss: 0.0842\n",
      "Batch 57/165 - Loss: 0.0836\n",
      "Batch 58/165 - Loss: 0.0459\n",
      "Batch 59/165 - Loss: 0.0099\n",
      "Batch 60/165 - Loss: 0.0177\n",
      "Batch 61/165 - Loss: 0.1321\n",
      "Batch 62/165 - Loss: 0.0070\n",
      "Batch 63/165 - Loss: 0.0508\n",
      "Batch 64/165 - Loss: 0.0984\n",
      "Batch 65/165 - Loss: 0.0030\n",
      "Batch 66/165 - Loss: 0.0405\n",
      "Batch 67/165 - Loss: 0.0270\n",
      "Batch 68/165 - Loss: 0.0114\n",
      "Batch 69/165 - Loss: 0.0031\n",
      "Batch 70/165 - Loss: 0.0061\n",
      "Batch 71/165 - Loss: 0.1010\n",
      "Batch 72/165 - Loss: 0.0113\n",
      "Batch 73/165 - Loss: 0.0122\n",
      "Batch 74/165 - Loss: 0.0156\n",
      "Batch 75/165 - Loss: 0.0080\n",
      "Batch 76/165 - Loss: 0.0076\n",
      "Batch 77/165 - Loss: 0.0028\n",
      "Batch 78/165 - Loss: 0.0045\n",
      "Batch 79/165 - Loss: 0.0058\n",
      "Batch 80/165 - Loss: 0.1219\n",
      "Batch 81/165 - Loss: 0.0030\n",
      "Batch 82/165 - Loss: 0.0123\n",
      "Batch 83/165 - Loss: 0.0415\n",
      "Batch 84/165 - Loss: 0.0098\n",
      "Batch 85/165 - Loss: 0.0149\n",
      "Batch 86/165 - Loss: 0.0135\n",
      "Batch 87/165 - Loss: 0.0360\n",
      "Batch 88/165 - Loss: 0.0033\n",
      "Batch 89/165 - Loss: 0.1345\n",
      "Batch 90/165 - Loss: 0.0570\n",
      "Batch 91/165 - Loss: 0.0404\n",
      "Batch 92/165 - Loss: 0.0346\n",
      "Batch 93/165 - Loss: 0.0020\n",
      "Batch 94/165 - Loss: 0.1341\n",
      "Batch 95/165 - Loss: 0.0108\n",
      "Batch 96/165 - Loss: 0.2978\n",
      "Batch 97/165 - Loss: 0.0081\n",
      "Batch 98/165 - Loss: 0.0083\n",
      "Batch 99/165 - Loss: 0.0303\n",
      "Batch 100/165 - Loss: 0.0061\n",
      "Batch 101/165 - Loss: 0.0017\n",
      "Batch 102/165 - Loss: 0.0035\n",
      "Batch 103/165 - Loss: 0.0142\n",
      "Batch 104/165 - Loss: 0.0183\n",
      "Batch 105/165 - Loss: 0.0875\n",
      "Batch 106/165 - Loss: 0.0048\n",
      "Batch 107/165 - Loss: 0.0132\n",
      "Batch 108/165 - Loss: 0.0451\n",
      "Batch 109/165 - Loss: 0.0313\n",
      "Batch 110/165 - Loss: 0.0085\n",
      "Batch 111/165 - Loss: 0.0021\n",
      "Batch 112/165 - Loss: 0.0534\n",
      "Batch 113/165 - Loss: 0.2638\n",
      "Batch 114/165 - Loss: 0.0892\n",
      "Batch 115/165 - Loss: 0.0313\n",
      "Batch 116/165 - Loss: 0.3122\n",
      "Batch 117/165 - Loss: 0.0033\n",
      "Batch 118/165 - Loss: 0.0082\n",
      "Batch 119/165 - Loss: 0.0217\n",
      "Batch 120/165 - Loss: 0.0602\n",
      "Batch 121/165 - Loss: 0.0095\n",
      "Batch 122/165 - Loss: 0.0400\n",
      "Batch 123/165 - Loss: 0.0078\n",
      "Batch 124/165 - Loss: 0.0136\n",
      "Batch 125/165 - Loss: 0.0323\n",
      "Batch 126/165 - Loss: 0.0116\n",
      "Batch 127/165 - Loss: 0.0798\n",
      "Batch 128/165 - Loss: 0.0141\n",
      "Batch 129/165 - Loss: 0.0089\n",
      "Batch 130/165 - Loss: 0.0929\n",
      "Batch 131/165 - Loss: 0.0073\n",
      "Batch 132/165 - Loss: 0.0237\n",
      "Batch 133/165 - Loss: 0.0315\n",
      "Batch 134/165 - Loss: 0.0186\n",
      "Batch 135/165 - Loss: 0.0028\n",
      "Batch 136/165 - Loss: 0.0108\n",
      "Batch 137/165 - Loss: 0.0677\n",
      "Batch 138/165 - Loss: 0.0699\n",
      "Batch 139/165 - Loss: 0.0034\n",
      "Batch 140/165 - Loss: 0.0964\n",
      "Batch 141/165 - Loss: 0.0062\n",
      "Batch 142/165 - Loss: 0.0030\n",
      "Batch 143/165 - Loss: 0.0334\n",
      "Batch 144/165 - Loss: 0.0045\n",
      "Batch 145/165 - Loss: 0.0632\n",
      "Batch 146/165 - Loss: 0.0152\n",
      "Batch 147/165 - Loss: 0.0240\n",
      "Batch 148/165 - Loss: 0.0743\n",
      "Batch 149/165 - Loss: 0.0047\n",
      "Batch 150/165 - Loss: 0.0782\n",
      "Batch 151/165 - Loss: 0.0342\n",
      "Batch 152/165 - Loss: 0.0216\n",
      "Batch 153/165 - Loss: 0.0140\n",
      "Batch 154/165 - Loss: 0.0977\n",
      "Batch 155/165 - Loss: 0.0052\n",
      "Batch 156/165 - Loss: 0.0154\n",
      "Batch 157/165 - Loss: 0.0145\n",
      "Batch 158/165 - Loss: 0.0561\n",
      "Batch 159/165 - Loss: 0.0043\n",
      "Batch 160/165 - Loss: 0.0189\n",
      "Batch 161/165 - Loss: 0.0046\n",
      "Batch 162/165 - Loss: 0.0165\n",
      "Batch 163/165 - Loss: 0.0111\n",
      "Batch 164/165 - Loss: 0.0067\n",
      "Batch 165/165 - Loss: 0.0157\n",
      "Epoch 6/15 | Train Acc: 0.986 | Val Acc: 0.995\n",
      "Batch 1/165 - Loss: 0.0152\n",
      "Batch 2/165 - Loss: 0.0021\n",
      "Batch 3/165 - Loss: 0.0042\n",
      "Batch 4/165 - Loss: 0.0079\n",
      "Batch 5/165 - Loss: 0.0294\n",
      "Batch 6/165 - Loss: 0.1700\n",
      "Batch 7/165 - Loss: 0.2114\n",
      "Batch 8/165 - Loss: 0.0050\n",
      "Batch 9/165 - Loss: 0.0072\n",
      "Batch 10/165 - Loss: 0.0015\n",
      "Batch 11/165 - Loss: 0.0024\n",
      "Batch 12/165 - Loss: 0.0227\n",
      "Batch 13/165 - Loss: 0.0024\n",
      "Batch 14/165 - Loss: 0.1209\n",
      "Batch 15/165 - Loss: 0.0022\n",
      "Batch 16/165 - Loss: 0.0031\n",
      "Batch 17/165 - Loss: 0.0308\n",
      "Batch 18/165 - Loss: 0.0054\n",
      "Batch 19/165 - Loss: 0.0043\n",
      "Batch 20/165 - Loss: 0.2829\n",
      "Batch 21/165 - Loss: 0.0042\n",
      "Batch 22/165 - Loss: 0.0199\n",
      "Batch 23/165 - Loss: 0.0396\n",
      "Batch 24/165 - Loss: 0.0019\n",
      "Batch 25/165 - Loss: 0.0069\n",
      "Batch 26/165 - Loss: 0.0018\n",
      "Batch 27/165 - Loss: 0.0082\n",
      "Batch 28/165 - Loss: 0.0084\n",
      "Batch 29/165 - Loss: 0.0024\n",
      "Batch 30/165 - Loss: 0.0178\n",
      "Batch 31/165 - Loss: 0.0028\n",
      "Batch 32/165 - Loss: 0.0013\n",
      "Batch 33/165 - Loss: 0.0187\n",
      "Batch 34/165 - Loss: 0.1683\n",
      "Batch 35/165 - Loss: 0.0218\n",
      "Batch 36/165 - Loss: 0.0084\n",
      "Batch 37/165 - Loss: 0.1072\n",
      "Batch 38/165 - Loss: 0.0028\n",
      "Batch 39/165 - Loss: 0.0578\n",
      "Batch 40/165 - Loss: 0.0159\n",
      "Batch 41/165 - Loss: 0.0346\n",
      "Batch 42/165 - Loss: 0.0053\n",
      "Batch 43/165 - Loss: 0.0299\n",
      "Batch 44/165 - Loss: 0.0258\n",
      "Batch 45/165 - Loss: 0.0952\n",
      "Batch 46/165 - Loss: 0.0052\n",
      "Batch 47/165 - Loss: 0.0081\n",
      "Batch 48/165 - Loss: 0.0329\n",
      "Batch 49/165 - Loss: 0.0116\n",
      "Batch 50/165 - Loss: 0.0653\n",
      "Batch 51/165 - Loss: 0.0075\n",
      "Batch 52/165 - Loss: 0.1524\n",
      "Batch 53/165 - Loss: 0.0159\n",
      "Batch 54/165 - Loss: 0.0187\n",
      "Batch 55/165 - Loss: 0.0203\n",
      "Batch 56/165 - Loss: 0.0799\n",
      "Batch 57/165 - Loss: 0.0715\n",
      "Batch 58/165 - Loss: 0.0482\n",
      "Batch 59/165 - Loss: 0.0096\n",
      "Batch 60/165 - Loss: 0.0177\n",
      "Batch 61/165 - Loss: 0.1352\n",
      "Batch 62/165 - Loss: 0.0075\n",
      "Batch 63/165 - Loss: 0.0448\n",
      "Batch 64/165 - Loss: 0.0785\n",
      "Batch 65/165 - Loss: 0.0026\n",
      "Batch 66/165 - Loss: 0.0372\n",
      "Batch 67/165 - Loss: 0.0246\n",
      "Batch 68/165 - Loss: 0.0103\n",
      "Batch 69/165 - Loss: 0.0024\n",
      "Batch 70/165 - Loss: 0.0057\n",
      "Batch 71/165 - Loss: 0.0646\n",
      "Batch 72/165 - Loss: 0.0108\n",
      "Batch 73/165 - Loss: 0.0098\n",
      "Batch 74/165 - Loss: 0.0157\n",
      "Batch 75/165 - Loss: 0.0077\n",
      "Batch 76/165 - Loss: 0.0060\n",
      "Batch 77/165 - Loss: 0.0022\n",
      "Batch 78/165 - Loss: 0.0035\n",
      "Batch 79/165 - Loss: 0.0069\n",
      "Batch 80/165 - Loss: 0.1148\n",
      "Batch 81/165 - Loss: 0.0022\n",
      "Batch 82/165 - Loss: 0.0114\n",
      "Batch 83/165 - Loss: 0.0384\n",
      "Batch 84/165 - Loss: 0.0093\n",
      "Batch 85/165 - Loss: 0.0144\n",
      "Batch 86/165 - Loss: 0.0128\n",
      "Batch 87/165 - Loss: 0.0336\n",
      "Batch 88/165 - Loss: 0.0029\n",
      "Batch 89/165 - Loss: 0.1378\n",
      "Batch 90/165 - Loss: 0.0500\n",
      "Batch 91/165 - Loss: 0.0338\n",
      "Batch 92/165 - Loss: 0.0200\n",
      "Batch 93/165 - Loss: 0.0016\n",
      "Batch 94/165 - Loss: 0.1308\n",
      "Batch 95/165 - Loss: 0.0101\n",
      "Batch 96/165 - Loss: 0.2799\n",
      "Batch 97/165 - Loss: 0.0081\n",
      "Batch 98/165 - Loss: 0.0087\n",
      "Batch 99/165 - Loss: 0.0288\n",
      "Batch 100/165 - Loss: 0.0056\n",
      "Batch 101/165 - Loss: 0.0017\n",
      "Batch 102/165 - Loss: 0.0035\n",
      "Batch 103/165 - Loss: 0.0297\n",
      "Batch 104/165 - Loss: 0.0126\n",
      "Batch 105/165 - Loss: 0.0838\n",
      "Batch 106/165 - Loss: 0.0039\n",
      "Batch 107/165 - Loss: 0.0123\n",
      "Batch 108/165 - Loss: 0.0481\n",
      "Batch 109/165 - Loss: 0.0337\n",
      "Batch 110/165 - Loss: 0.0074\n",
      "Batch 111/165 - Loss: 0.0019\n",
      "Batch 112/165 - Loss: 0.0561\n",
      "Batch 113/165 - Loss: 0.2575\n",
      "Batch 114/165 - Loss: 0.0933\n",
      "Batch 115/165 - Loss: 0.0304\n",
      "Batch 116/165 - Loss: 0.3191\n",
      "Batch 117/165 - Loss: 0.0031\n",
      "Batch 118/165 - Loss: 0.0071\n",
      "Batch 119/165 - Loss: 0.0176\n",
      "Batch 120/165 - Loss: 0.0516\n",
      "Batch 121/165 - Loss: 0.0105\n",
      "Batch 122/165 - Loss: 0.0394\n",
      "Batch 123/165 - Loss: 0.0080\n",
      "Batch 124/165 - Loss: 0.0131\n",
      "Batch 125/165 - Loss: 0.0318\n",
      "Batch 126/165 - Loss: 0.0113\n",
      "Batch 127/165 - Loss: 0.0850\n",
      "Batch 128/165 - Loss: 0.0144\n",
      "Batch 129/165 - Loss: 0.0097\n",
      "Batch 130/165 - Loss: 0.1024\n",
      "Batch 131/165 - Loss: 0.0068\n",
      "Batch 132/165 - Loss: 0.0248\n",
      "Batch 133/165 - Loss: 0.0281\n",
      "Batch 134/165 - Loss: 0.0213\n",
      "Batch 135/165 - Loss: 0.0029\n",
      "Batch 136/165 - Loss: 0.0095\n",
      "Batch 137/165 - Loss: 0.0657\n",
      "Batch 138/165 - Loss: 0.0763\n",
      "Batch 139/165 - Loss: 0.0035\n",
      "Batch 140/165 - Loss: 0.1043\n",
      "Batch 141/165 - Loss: 0.0065\n",
      "Batch 142/165 - Loss: 0.0027\n",
      "Batch 143/165 - Loss: 0.0291\n",
      "Batch 144/165 - Loss: 0.0052\n",
      "Batch 145/165 - Loss: 0.0604\n",
      "Batch 146/165 - Loss: 0.0154\n",
      "Batch 147/165 - Loss: 0.0219\n",
      "Batch 148/165 - Loss: 0.0775\n",
      "Batch 149/165 - Loss: 0.0043\n",
      "Batch 150/165 - Loss: 0.0734\n",
      "Batch 151/165 - Loss: 0.0355\n",
      "Batch 152/165 - Loss: 0.0231\n",
      "Batch 153/165 - Loss: 0.0133\n",
      "Batch 154/165 - Loss: 0.1014\n",
      "Batch 155/165 - Loss: 0.0047\n",
      "Batch 156/165 - Loss: 0.0155\n",
      "Batch 157/165 - Loss: 0.0150\n",
      "Batch 158/165 - Loss: 0.0524\n",
      "Batch 159/165 - Loss: 0.0040\n",
      "Batch 160/165 - Loss: 0.0195\n",
      "Batch 161/165 - Loss: 0.0040\n",
      "Batch 162/165 - Loss: 0.0145\n",
      "Batch 163/165 - Loss: 0.0102\n",
      "Batch 164/165 - Loss: 0.0064\n",
      "Batch 165/165 - Loss: 0.0166\n",
      "Epoch 7/15 | Train Acc: 0.985 | Val Acc: 0.995\n",
      "Batch 1/165 - Loss: 0.0159\n",
      "Batch 2/165 - Loss: 0.0018\n",
      "Batch 3/165 - Loss: 0.0044\n",
      "Batch 4/165 - Loss: 0.0103\n",
      "Batch 5/165 - Loss: 0.0291\n",
      "Batch 6/165 - Loss: 0.1623\n",
      "Batch 7/165 - Loss: 0.2136\n",
      "Batch 8/165 - Loss: 0.0048\n",
      "Batch 9/165 - Loss: 0.0077\n",
      "Batch 10/165 - Loss: 0.0013\n",
      "Batch 11/165 - Loss: 0.0023\n",
      "Batch 12/165 - Loss: 0.0235\n",
      "Batch 13/165 - Loss: 0.0020\n",
      "Batch 14/165 - Loss: 0.1258\n",
      "Batch 15/165 - Loss: 0.0020\n",
      "Batch 16/165 - Loss: 0.0026\n",
      "Batch 17/165 - Loss: 0.0279\n",
      "Batch 18/165 - Loss: 0.0050\n",
      "Batch 19/165 - Loss: 0.0038\n",
      "Batch 20/165 - Loss: 0.2745\n",
      "Batch 21/165 - Loss: 0.0038\n",
      "Batch 22/165 - Loss: 0.0150\n",
      "Batch 23/165 - Loss: 0.0394\n",
      "Batch 24/165 - Loss: 0.0018\n",
      "Batch 25/165 - Loss: 0.0071\n",
      "Batch 26/165 - Loss: 0.0016\n",
      "Batch 27/165 - Loss: 0.0109\n",
      "Batch 28/165 - Loss: 0.0084\n",
      "Batch 29/165 - Loss: 0.0021\n",
      "Batch 30/165 - Loss: 0.0157\n",
      "Batch 31/165 - Loss: 0.0024\n",
      "Batch 32/165 - Loss: 0.0012\n",
      "Batch 33/165 - Loss: 0.0210\n",
      "Batch 34/165 - Loss: 0.1533\n",
      "Batch 35/165 - Loss: 0.0224\n",
      "Batch 36/165 - Loss: 0.0101\n",
      "Batch 37/165 - Loss: 0.1211\n",
      "Batch 38/165 - Loss: 0.0025\n",
      "Batch 39/165 - Loss: 0.0557\n",
      "Batch 40/165 - Loss: 0.0143\n",
      "Batch 41/165 - Loss: 0.0366\n",
      "Batch 42/165 - Loss: 0.0054\n",
      "Batch 43/165 - Loss: 0.0275\n",
      "Batch 44/165 - Loss: 0.0279\n",
      "Batch 45/165 - Loss: 0.0913\n",
      "Batch 46/165 - Loss: 0.0048\n",
      "Batch 47/165 - Loss: 0.0085\n",
      "Batch 48/165 - Loss: 0.0337\n",
      "Batch 49/165 - Loss: 0.0111\n",
      "Batch 50/165 - Loss: 0.0579\n",
      "Batch 51/165 - Loss: 0.0074\n",
      "Batch 52/165 - Loss: 0.1885\n",
      "Batch 53/165 - Loss: 0.0179\n",
      "Batch 54/165 - Loss: 0.0156\n",
      "Batch 55/165 - Loss: 0.0196\n",
      "Batch 56/165 - Loss: 0.0890\n",
      "Batch 57/165 - Loss: 0.0652\n",
      "Batch 58/165 - Loss: 0.0472\n",
      "Batch 59/165 - Loss: 0.0093\n",
      "Batch 60/165 - Loss: 0.0170\n",
      "Batch 61/165 - Loss: 0.1378\n",
      "Batch 62/165 - Loss: 0.0075\n",
      "Batch 63/165 - Loss: 0.0426\n",
      "Batch 64/165 - Loss: 0.0836\n",
      "Batch 65/165 - Loss: 0.0025\n",
      "Batch 66/165 - Loss: 0.0356\n",
      "Batch 67/165 - Loss: 0.0238\n",
      "Batch 68/165 - Loss: 0.0101\n",
      "Batch 69/165 - Loss: 0.0027\n",
      "Batch 70/165 - Loss: 0.0061\n",
      "Batch 71/165 - Loss: 0.0576\n",
      "Batch 72/165 - Loss: 0.0122\n",
      "Batch 73/165 - Loss: 0.0089\n",
      "Batch 74/165 - Loss: 0.0199\n",
      "Batch 75/165 - Loss: 0.0110\n",
      "Batch 76/165 - Loss: 0.0056\n",
      "Batch 77/165 - Loss: 0.0018\n",
      "Batch 78/165 - Loss: 0.0033\n",
      "Batch 79/165 - Loss: 0.0076\n",
      "Batch 80/165 - Loss: 0.1117\n",
      "Batch 81/165 - Loss: 0.0019\n",
      "Batch 82/165 - Loss: 0.0116\n",
      "Batch 83/165 - Loss: 0.0400\n",
      "Batch 84/165 - Loss: 0.0088\n",
      "Batch 85/165 - Loss: 0.0143\n",
      "Batch 86/165 - Loss: 0.0140\n",
      "Batch 87/165 - Loss: 0.0305\n",
      "Batch 88/165 - Loss: 0.0028\n",
      "Batch 89/165 - Loss: 0.1334\n",
      "Batch 90/165 - Loss: 0.0481\n",
      "Batch 91/165 - Loss: 0.0313\n",
      "Batch 92/165 - Loss: 0.0089\n",
      "Batch 93/165 - Loss: 0.0015\n",
      "Batch 94/165 - Loss: 0.1272\n",
      "Batch 95/165 - Loss: 0.0096\n",
      "Batch 96/165 - Loss: 0.2799\n",
      "Batch 97/165 - Loss: 0.0074\n",
      "Batch 98/165 - Loss: 0.0099\n",
      "Batch 99/165 - Loss: 0.0259\n",
      "Batch 100/165 - Loss: 0.0050\n",
      "Batch 101/165 - Loss: 0.0020\n",
      "Batch 102/165 - Loss: 0.0040\n",
      "Batch 103/165 - Loss: 0.0279\n",
      "Batch 104/165 - Loss: 0.0093\n",
      "Batch 105/165 - Loss: 0.0746\n",
      "Batch 106/165 - Loss: 0.0027\n",
      "Batch 107/165 - Loss: 0.0099\n",
      "Batch 108/165 - Loss: 0.0486\n",
      "Batch 109/165 - Loss: 0.0293\n",
      "Batch 110/165 - Loss: 0.0061\n",
      "Batch 111/165 - Loss: 0.0018\n",
      "Batch 112/165 - Loss: 0.0551\n",
      "Batch 113/165 - Loss: 0.2712\n",
      "Batch 114/165 - Loss: 0.0975\n",
      "Batch 115/165 - Loss: 0.0227\n",
      "Batch 116/165 - Loss: 0.3296\n",
      "Batch 117/165 - Loss: 0.0023\n",
      "Batch 118/165 - Loss: 0.0060\n",
      "Batch 119/165 - Loss: 0.0138\n",
      "Batch 120/165 - Loss: 0.0416\n",
      "Batch 121/165 - Loss: 0.0113\n",
      "Batch 122/165 - Loss: 0.0312\n",
      "Batch 123/165 - Loss: 0.0064\n",
      "Batch 124/165 - Loss: 0.0079\n",
      "Batch 125/165 - Loss: 0.0302\n",
      "Batch 126/165 - Loss: 0.0107\n",
      "Batch 127/165 - Loss: 0.0829\n",
      "Batch 128/165 - Loss: 0.0151\n",
      "Batch 129/165 - Loss: 0.0097\n",
      "Batch 130/165 - Loss: 0.1088\n",
      "Batch 131/165 - Loss: 0.0065\n",
      "Batch 132/165 - Loss: 0.0282\n",
      "Batch 133/165 - Loss: 0.0258\n",
      "Batch 134/165 - Loss: 0.0235\n",
      "Batch 135/165 - Loss: 0.0028\n",
      "Batch 136/165 - Loss: 0.0084\n",
      "Batch 137/165 - Loss: 0.0595\n",
      "Batch 138/165 - Loss: 0.0669\n",
      "Batch 139/165 - Loss: 0.0035\n",
      "Batch 140/165 - Loss: 0.1151\n",
      "Batch 141/165 - Loss: 0.0068\n",
      "Batch 142/165 - Loss: 0.0025\n",
      "Batch 143/165 - Loss: 0.0263\n",
      "Batch 144/165 - Loss: 0.0042\n",
      "Batch 145/165 - Loss: 0.0628\n",
      "Batch 146/165 - Loss: 0.0134\n",
      "Batch 147/165 - Loss: 0.0240\n",
      "Batch 148/165 - Loss: 0.0795\n",
      "Batch 149/165 - Loss: 0.0039\n",
      "Batch 150/165 - Loss: 0.0650\n",
      "Batch 151/165 - Loss: 0.0282\n",
      "Batch 152/165 - Loss: 0.0193\n",
      "Batch 153/165 - Loss: 0.0120\n",
      "Batch 154/165 - Loss: 0.1056\n",
      "Batch 155/165 - Loss: 0.0044\n",
      "Batch 156/165 - Loss: 0.0160\n",
      "Batch 157/165 - Loss: 0.0163\n",
      "Batch 158/165 - Loss: 0.0513\n",
      "Batch 159/165 - Loss: 0.0038\n",
      "Batch 160/165 - Loss: 0.0194\n",
      "Batch 161/165 - Loss: 0.0028\n",
      "Batch 162/165 - Loss: 0.0148\n",
      "Batch 163/165 - Loss: 0.0102\n",
      "Batch 164/165 - Loss: 0.0063\n",
      "Batch 165/165 - Loss: 0.0172\n",
      "Epoch 8/15 | Train Acc: 0.986 | Val Acc: 0.994\n",
      "Batch 1/165 - Loss: 0.0163\n",
      "Batch 2/165 - Loss: 0.0019\n",
      "Batch 3/165 - Loss: 0.0041\n",
      "Batch 4/165 - Loss: 0.0075\n",
      "Batch 5/165 - Loss: 0.0265\n",
      "Batch 6/165 - Loss: 0.1548\n",
      "Batch 7/165 - Loss: 0.2078\n",
      "Batch 8/165 - Loss: 0.0050\n",
      "Batch 9/165 - Loss: 0.0081\n",
      "Batch 10/165 - Loss: 0.0012\n",
      "Batch 11/165 - Loss: 0.0023\n",
      "Batch 12/165 - Loss: 0.0249\n",
      "Batch 13/165 - Loss: 0.0017\n",
      "Batch 14/165 - Loss: 0.1260\n",
      "Batch 15/165 - Loss: 0.0019\n",
      "Batch 16/165 - Loss: 0.0020\n",
      "Batch 17/165 - Loss: 0.0221\n",
      "Batch 18/165 - Loss: 0.0047\n",
      "Batch 19/165 - Loss: 0.0032\n",
      "Batch 20/165 - Loss: 0.2647\n",
      "Batch 21/165 - Loss: 0.0037\n",
      "Batch 22/165 - Loss: 0.0127\n",
      "Batch 23/165 - Loss: 0.0368\n",
      "Batch 24/165 - Loss: 0.0017\n",
      "Batch 25/165 - Loss: 0.0076\n",
      "Batch 26/165 - Loss: 0.0014\n",
      "Batch 27/165 - Loss: 0.0117\n",
      "Batch 28/165 - Loss: 0.0082\n",
      "Batch 29/165 - Loss: 0.0021\n",
      "Batch 30/165 - Loss: 0.0145\n",
      "Batch 31/165 - Loss: 0.0022\n",
      "Batch 32/165 - Loss: 0.0012\n",
      "Batch 33/165 - Loss: 0.0212\n",
      "Batch 34/165 - Loss: 0.1406\n",
      "Batch 35/165 - Loss: 0.0235\n",
      "Batch 36/165 - Loss: 0.0135\n",
      "Batch 37/165 - Loss: 0.1304\n",
      "Batch 38/165 - Loss: 0.0023\n",
      "Batch 39/165 - Loss: 0.0517\n",
      "Batch 40/165 - Loss: 0.0130\n",
      "Batch 41/165 - Loss: 0.0411\n",
      "Batch 42/165 - Loss: 0.0049\n",
      "Batch 43/165 - Loss: 0.0252\n",
      "Batch 44/165 - Loss: 0.0283\n",
      "Batch 45/165 - Loss: 0.0903\n",
      "Batch 46/165 - Loss: 0.0042\n",
      "Batch 47/165 - Loss: 0.0097\n",
      "Batch 48/165 - Loss: 0.0322\n",
      "Batch 49/165 - Loss: 0.0103\n",
      "Batch 50/165 - Loss: 0.0540\n",
      "Batch 51/165 - Loss: 0.0077\n",
      "Batch 52/165 - Loss: 0.2107\n",
      "Batch 53/165 - Loss: 0.0228\n",
      "Batch 54/165 - Loss: 0.0170\n",
      "Batch 55/165 - Loss: 0.0192\n",
      "Batch 56/165 - Loss: 0.1008\n",
      "Batch 57/165 - Loss: 0.0602\n",
      "Batch 58/165 - Loss: 0.0468\n",
      "Batch 59/165 - Loss: 0.0093\n",
      "Batch 60/165 - Loss: 0.0170\n",
      "Batch 61/165 - Loss: 0.1488\n",
      "Batch 62/165 - Loss: 0.0078\n",
      "Batch 63/165 - Loss: 0.0338\n",
      "Batch 64/165 - Loss: 0.0837\n",
      "Batch 65/165 - Loss: 0.0026\n",
      "Batch 66/165 - Loss: 0.0334\n",
      "Batch 67/165 - Loss: 0.0228\n",
      "Batch 68/165 - Loss: 0.0099\n",
      "Batch 69/165 - Loss: 0.0027\n",
      "Batch 70/165 - Loss: 0.0055\n",
      "Batch 71/165 - Loss: 0.0632\n",
      "Batch 72/165 - Loss: 0.0126\n",
      "Batch 73/165 - Loss: 0.0080\n",
      "Batch 74/165 - Loss: 0.0196\n",
      "Batch 75/165 - Loss: 0.0105\n",
      "Batch 76/165 - Loss: 0.0050\n",
      "Batch 77/165 - Loss: 0.0017\n",
      "Batch 78/165 - Loss: 0.0032\n",
      "Batch 79/165 - Loss: 0.0088\n",
      "Batch 80/165 - Loss: 0.1025\n",
      "Batch 81/165 - Loss: 0.0020\n",
      "Batch 82/165 - Loss: 0.0118\n",
      "Batch 83/165 - Loss: 0.0369\n",
      "Batch 84/165 - Loss: 0.0084\n",
      "Batch 85/165 - Loss: 0.0143\n",
      "Batch 86/165 - Loss: 0.0139\n",
      "Batch 87/165 - Loss: 0.0299\n",
      "Batch 88/165 - Loss: 0.0027\n",
      "Batch 89/165 - Loss: 0.1363\n",
      "Batch 90/165 - Loss: 0.0520\n",
      "Batch 91/165 - Loss: 0.0340\n",
      "Batch 92/165 - Loss: 0.0128\n",
      "Batch 93/165 - Loss: 0.0015\n",
      "Batch 94/165 - Loss: 0.1168\n",
      "Batch 95/165 - Loss: 0.0100\n",
      "Batch 96/165 - Loss: 0.2531\n",
      "Batch 97/165 - Loss: 0.0083\n",
      "Batch 98/165 - Loss: 0.0094\n",
      "Batch 99/165 - Loss: 0.0251\n",
      "Batch 100/165 - Loss: 0.0045\n",
      "Batch 101/165 - Loss: 0.0017\n",
      "Batch 102/165 - Loss: 0.0041\n",
      "Batch 103/165 - Loss: 0.0069\n",
      "Batch 104/165 - Loss: 0.0100\n",
      "Batch 105/165 - Loss: 0.0643\n",
      "Batch 106/165 - Loss: 0.0019\n",
      "Batch 107/165 - Loss: 0.0078\n",
      "Batch 108/165 - Loss: 0.0489\n",
      "Batch 109/165 - Loss: 0.0225\n",
      "Batch 110/165 - Loss: 0.0063\n",
      "Batch 111/165 - Loss: 0.0018\n",
      "Batch 112/165 - Loss: 0.0511\n",
      "Batch 113/165 - Loss: 0.2379\n",
      "Batch 114/165 - Loss: 0.0976\n",
      "Batch 115/165 - Loss: 0.0125\n",
      "Batch 116/165 - Loss: 0.3322\n",
      "Batch 117/165 - Loss: 0.0020\n",
      "Batch 118/165 - Loss: 0.0058\n",
      "Batch 119/165 - Loss: 0.0124\n",
      "Batch 120/165 - Loss: 0.0368\n",
      "Batch 121/165 - Loss: 0.0112\n",
      "Batch 122/165 - Loss: 0.0240\n",
      "Batch 123/165 - Loss: 0.0057\n",
      "Batch 124/165 - Loss: 0.0120\n",
      "Batch 125/165 - Loss: 0.0280\n",
      "Batch 126/165 - Loss: 0.0099\n",
      "Batch 127/165 - Loss: 0.0623\n",
      "Batch 128/165 - Loss: 0.0151\n",
      "Batch 129/165 - Loss: 0.0090\n",
      "Batch 130/165 - Loss: 0.1013\n",
      "Batch 131/165 - Loss: 0.0062\n",
      "Batch 132/165 - Loss: 0.0285\n",
      "Batch 133/165 - Loss: 0.0250\n",
      "Batch 134/165 - Loss: 0.0223\n",
      "Batch 135/165 - Loss: 0.0024\n",
      "Batch 136/165 - Loss: 0.0080\n",
      "Batch 137/165 - Loss: 0.0581\n",
      "Batch 138/165 - Loss: 0.0573\n",
      "Batch 139/165 - Loss: 0.0034\n",
      "Batch 140/165 - Loss: 0.1234\n",
      "Batch 141/165 - Loss: 0.0078\n",
      "Batch 142/165 - Loss: 0.0022\n",
      "Batch 143/165 - Loss: 0.0238\n",
      "Batch 144/165 - Loss: 0.0035\n",
      "Batch 145/165 - Loss: 0.0514\n",
      "Batch 146/165 - Loss: 0.0128\n",
      "Batch 147/165 - Loss: 0.0224\n",
      "Batch 148/165 - Loss: 0.0782\n",
      "Batch 149/165 - Loss: 0.0037\n",
      "Batch 150/165 - Loss: 0.0650\n",
      "Batch 151/165 - Loss: 0.0233\n",
      "Batch 152/165 - Loss: 0.0155\n",
      "Batch 153/165 - Loss: 0.0114\n",
      "Batch 154/165 - Loss: 0.1061\n",
      "Batch 155/165 - Loss: 0.0041\n",
      "Batch 156/165 - Loss: 0.0164\n",
      "Batch 157/165 - Loss: 0.0165\n",
      "Batch 158/165 - Loss: 0.0551\n",
      "Batch 159/165 - Loss: 0.0037\n",
      "Batch 160/165 - Loss: 0.0198\n",
      "Batch 161/165 - Loss: 0.0030\n",
      "Batch 162/165 - Loss: 0.0171\n",
      "Batch 163/165 - Loss: 0.0103\n",
      "Batch 164/165 - Loss: 0.0079\n",
      "Batch 165/165 - Loss: 0.0203\n",
      "Epoch 9/15 | Train Acc: 0.986 | Val Acc: 0.992\n",
      "Batch 1/165 - Loss: 0.0163\n",
      "Batch 2/165 - Loss: 0.0020\n",
      "Batch 3/165 - Loss: 0.0038\n",
      "Batch 4/165 - Loss: 0.0057\n",
      "Batch 5/165 - Loss: 0.0263\n",
      "Batch 6/165 - Loss: 0.1545\n",
      "Batch 7/165 - Loss: 0.2067\n",
      "Batch 8/165 - Loss: 0.0045\n",
      "Batch 9/165 - Loss: 0.0073\n",
      "Batch 10/165 - Loss: 0.0011\n",
      "Batch 11/165 - Loss: 0.0022\n",
      "Batch 12/165 - Loss: 0.0259\n",
      "Batch 13/165 - Loss: 0.0016\n",
      "Batch 14/165 - Loss: 0.1306\n",
      "Batch 15/165 - Loss: 0.0018\n",
      "Batch 16/165 - Loss: 0.0019\n",
      "Batch 17/165 - Loss: 0.0214\n",
      "Batch 18/165 - Loss: 0.0045\n",
      "Batch 19/165 - Loss: 0.0032\n",
      "Batch 20/165 - Loss: 0.2575\n",
      "Batch 21/165 - Loss: 0.0035\n",
      "Batch 22/165 - Loss: 0.0136\n",
      "Batch 23/165 - Loss: 0.0352\n",
      "Batch 24/165 - Loss: 0.0015\n",
      "Batch 25/165 - Loss: 0.0079\n",
      "Batch 26/165 - Loss: 0.0013\n",
      "Batch 27/165 - Loss: 0.0108\n",
      "Batch 28/165 - Loss: 0.0086\n",
      "Batch 29/165 - Loss: 0.0023\n",
      "Batch 30/165 - Loss: 0.0134\n",
      "Batch 31/165 - Loss: 0.0022\n",
      "Batch 32/165 - Loss: 0.0011\n",
      "Batch 33/165 - Loss: 0.0202\n",
      "Batch 34/165 - Loss: 0.1284\n",
      "Batch 35/165 - Loss: 0.0241\n",
      "Batch 36/165 - Loss: 0.0128\n",
      "Batch 37/165 - Loss: 0.1351\n",
      "Batch 38/165 - Loss: 0.0022\n",
      "Batch 39/165 - Loss: 0.0498\n",
      "Batch 40/165 - Loss: 0.0118\n",
      "Batch 41/165 - Loss: 0.0433\n",
      "Batch 42/165 - Loss: 0.0046\n",
      "Batch 43/165 - Loss: 0.0247\n",
      "Batch 44/165 - Loss: 0.0273\n",
      "Batch 45/165 - Loss: 0.0896\n",
      "Batch 46/165 - Loss: 0.0035\n",
      "Batch 47/165 - Loss: 0.0090\n",
      "Batch 48/165 - Loss: 0.0300\n",
      "Batch 49/165 - Loss: 0.0085\n",
      "Batch 50/165 - Loss: 0.0618\n",
      "Batch 51/165 - Loss: 0.0051\n",
      "Batch 52/165 - Loss: 0.1808\n",
      "Batch 53/165 - Loss: 0.0176\n",
      "Batch 54/165 - Loss: 0.0223\n",
      "Batch 55/165 - Loss: 0.0177\n",
      "Batch 56/165 - Loss: 0.0941\n",
      "Batch 57/165 - Loss: 0.0629\n",
      "Batch 58/165 - Loss: 0.0488\n",
      "Batch 59/165 - Loss: 0.0090\n",
      "Batch 60/165 - Loss: 0.0171\n",
      "Batch 61/165 - Loss: 0.1605\n",
      "Batch 62/165 - Loss: 0.0081\n",
      "Batch 63/165 - Loss: 0.0294\n",
      "Batch 64/165 - Loss: 0.0488\n",
      "Batch 65/165 - Loss: 0.0023\n",
      "Batch 66/165 - Loss: 0.0314\n",
      "Batch 67/165 - Loss: 0.0207\n",
      "Batch 68/165 - Loss: 0.0091\n",
      "Batch 69/165 - Loss: 0.0015\n",
      "Batch 70/165 - Loss: 0.0055\n",
      "Batch 71/165 - Loss: 0.0338\n",
      "Batch 72/165 - Loss: 0.0099\n",
      "Batch 73/165 - Loss: 0.0079\n",
      "Batch 74/165 - Loss: 0.0177\n",
      "Batch 75/165 - Loss: 0.0084\n",
      "Batch 76/165 - Loss: 0.0048\n",
      "Batch 77/165 - Loss: 0.0016\n",
      "Batch 78/165 - Loss: 0.0029\n",
      "Batch 79/165 - Loss: 0.0104\n",
      "Batch 80/165 - Loss: 0.0971\n",
      "Batch 81/165 - Loss: 0.0017\n",
      "Batch 82/165 - Loss: 0.0122\n",
      "Batch 83/165 - Loss: 0.0339\n",
      "Batch 84/165 - Loss: 0.0081\n",
      "Batch 85/165 - Loss: 0.0150\n",
      "Batch 86/165 - Loss: 0.0142\n",
      "Batch 87/165 - Loss: 0.0299\n",
      "Batch 88/165 - Loss: 0.0027\n",
      "Batch 89/165 - Loss: 0.1406\n",
      "Batch 90/165 - Loss: 0.0434\n",
      "Batch 91/165 - Loss: 0.0315\n",
      "Batch 92/165 - Loss: 0.0057\n",
      "Batch 93/165 - Loss: 0.0014\n",
      "Batch 94/165 - Loss: 0.1063\n",
      "Batch 95/165 - Loss: 0.0074\n",
      "Batch 96/165 - Loss: 0.2826\n",
      "Batch 97/165 - Loss: 0.0077\n",
      "Batch 98/165 - Loss: 0.0098\n",
      "Batch 99/165 - Loss: 0.0237\n",
      "Batch 100/165 - Loss: 0.0036\n",
      "Batch 101/165 - Loss: 0.0028\n",
      "Batch 102/165 - Loss: 0.0045\n",
      "Batch 103/165 - Loss: 0.0415\n",
      "Batch 104/165 - Loss: 0.0107\n",
      "Batch 105/165 - Loss: 0.0597\n",
      "Batch 106/165 - Loss: 0.0043\n",
      "Batch 107/165 - Loss: 0.0206\n",
      "Batch 108/165 - Loss: 0.0761\n",
      "Batch 109/165 - Loss: 0.0343\n",
      "Batch 110/165 - Loss: 0.0068\n",
      "Batch 111/165 - Loss: 0.0019\n",
      "Batch 112/165 - Loss: 0.0530\n",
      "Batch 113/165 - Loss: 0.2651\n",
      "Batch 114/165 - Loss: 0.1061\n",
      "Batch 115/165 - Loss: 0.0318\n",
      "Batch 116/165 - Loss: 0.3479\n",
      "Batch 117/165 - Loss: 0.0023\n",
      "Batch 118/165 - Loss: 0.0047\n",
      "Batch 119/165 - Loss: 0.0102\n",
      "Batch 120/165 - Loss: 0.0250\n",
      "Batch 121/165 - Loss: 0.0111\n",
      "Batch 122/165 - Loss: 0.0246\n",
      "Batch 123/165 - Loss: 0.0069\n",
      "Batch 124/165 - Loss: 0.0084\n",
      "Batch 125/165 - Loss: 0.0237\n",
      "Batch 126/165 - Loss: 0.0110\n",
      "Batch 127/165 - Loss: 0.0801\n",
      "Batch 128/165 - Loss: 0.0125\n",
      "Batch 129/165 - Loss: 0.0099\n",
      "Batch 130/165 - Loss: 0.1102\n",
      "Batch 131/165 - Loss: 0.0060\n",
      "Batch 132/165 - Loss: 0.0294\n",
      "Batch 133/165 - Loss: 0.0224\n",
      "Batch 134/165 - Loss: 0.0297\n",
      "Batch 135/165 - Loss: 0.0034\n",
      "Batch 136/165 - Loss: 0.0073\n",
      "Batch 137/165 - Loss: 0.0525\n",
      "Batch 138/165 - Loss: 0.0576\n",
      "Batch 139/165 - Loss: 0.0044\n",
      "Batch 140/165 - Loss: 0.1350\n",
      "Batch 141/165 - Loss: 0.0082\n",
      "Batch 142/165 - Loss: 0.0026\n",
      "Batch 143/165 - Loss: 0.0200\n",
      "Batch 144/165 - Loss: 0.0042\n",
      "Batch 145/165 - Loss: 0.0530\n",
      "Batch 146/165 - Loss: 0.0136\n",
      "Batch 147/165 - Loss: 0.0221\n",
      "Batch 148/165 - Loss: 0.0825\n",
      "Batch 149/165 - Loss: 0.0039\n",
      "Batch 150/165 - Loss: 0.0594\n",
      "Batch 151/165 - Loss: 0.0245\n",
      "Batch 152/165 - Loss: 0.0177\n",
      "Batch 153/165 - Loss: 0.0120\n",
      "Batch 154/165 - Loss: 0.1089\n",
      "Batch 155/165 - Loss: 0.0040\n",
      "Batch 156/165 - Loss: 0.0164\n",
      "Batch 157/165 - Loss: 0.0186\n",
      "Batch 158/165 - Loss: 0.0529\n",
      "Batch 159/165 - Loss: 0.0033\n",
      "Batch 160/165 - Loss: 0.0195\n",
      "Batch 161/165 - Loss: 0.0027\n",
      "Batch 162/165 - Loss: 0.0154\n",
      "Batch 163/165 - Loss: 0.0096\n",
      "Batch 164/165 - Loss: 0.0091\n",
      "Batch 165/165 - Loss: 0.0252\n",
      "Epoch 10/15 | Train Acc: 0.986 | Val Acc: 0.996\n",
      "Batch 1/165 - Loss: 0.0167\n",
      "Batch 2/165 - Loss: 0.0013\n",
      "Batch 3/165 - Loss: 0.0038\n",
      "Batch 4/165 - Loss: 0.0056\n",
      "Batch 5/165 - Loss: 0.0254\n",
      "Batch 6/165 - Loss: 0.1338\n",
      "Batch 7/165 - Loss: 0.2134\n",
      "Batch 8/165 - Loss: 0.0045\n",
      "Batch 9/165 - Loss: 0.0071\n",
      "Batch 10/165 - Loss: 0.0011\n",
      "Batch 11/165 - Loss: 0.0024\n",
      "Batch 12/165 - Loss: 0.0300\n",
      "Batch 13/165 - Loss: 0.0018\n",
      "Batch 14/165 - Loss: 0.1400\n",
      "Batch 15/165 - Loss: 0.0018\n",
      "Batch 16/165 - Loss: 0.0018\n",
      "Batch 17/165 - Loss: 0.0183\n",
      "Batch 18/165 - Loss: 0.0041\n",
      "Batch 19/165 - Loss: 0.0028\n",
      "Batch 20/165 - Loss: 0.2500\n",
      "Batch 21/165 - Loss: 0.0034\n",
      "Batch 22/165 - Loss: 0.0122\n",
      "Batch 23/165 - Loss: 0.0353\n",
      "Batch 24/165 - Loss: 0.0014\n",
      "Batch 25/165 - Loss: 0.0079\n",
      "Batch 26/165 - Loss: 0.0013\n",
      "Batch 27/165 - Loss: 0.0106\n",
      "Batch 28/165 - Loss: 0.0084\n",
      "Batch 29/165 - Loss: 0.0025\n",
      "Batch 30/165 - Loss: 0.0123\n",
      "Batch 31/165 - Loss: 0.0020\n",
      "Batch 32/165 - Loss: 0.0011\n",
      "Batch 33/165 - Loss: 0.0240\n",
      "Batch 34/165 - Loss: 0.1204\n",
      "Batch 35/165 - Loss: 0.0243\n",
      "Batch 36/165 - Loss: 0.0157\n",
      "Batch 37/165 - Loss: 0.1380\n",
      "Batch 38/165 - Loss: 0.0020\n",
      "Batch 39/165 - Loss: 0.0518\n",
      "Batch 40/165 - Loss: 0.0106\n",
      "Batch 41/165 - Loss: 0.0413\n",
      "Batch 42/165 - Loss: 0.0048\n",
      "Batch 43/165 - Loss: 0.0234\n",
      "Batch 44/165 - Loss: 0.0304\n",
      "Batch 45/165 - Loss: 0.0843\n",
      "Batch 46/165 - Loss: 0.0038\n",
      "Batch 47/165 - Loss: 0.0110\n",
      "Batch 48/165 - Loss: 0.0338\n",
      "Batch 49/165 - Loss: 0.0080\n",
      "Batch 50/165 - Loss: 0.0440\n",
      "Batch 51/165 - Loss: 0.0066\n",
      "Batch 52/165 - Loss: 0.2230\n",
      "Batch 53/165 - Loss: 0.0235\n",
      "Batch 54/165 - Loss: 0.0130\n",
      "Batch 55/165 - Loss: 0.0169\n",
      "Batch 56/165 - Loss: 0.1018\n",
      "Batch 57/165 - Loss: 0.0583\n",
      "Batch 58/165 - Loss: 0.0456\n",
      "Batch 59/165 - Loss: 0.0084\n",
      "Batch 60/165 - Loss: 0.0160\n",
      "Batch 61/165 - Loss: 0.1532\n",
      "Batch 62/165 - Loss: 0.0074\n",
      "Batch 63/165 - Loss: 0.0306\n",
      "Batch 64/165 - Loss: 0.0556\n",
      "Batch 65/165 - Loss: 0.0021\n",
      "Batch 66/165 - Loss: 0.0302\n",
      "Batch 67/165 - Loss: 0.0196\n",
      "Batch 68/165 - Loss: 0.0093\n",
      "Batch 69/165 - Loss: 0.0024\n",
      "Batch 70/165 - Loss: 0.0063\n",
      "Batch 71/165 - Loss: 0.0536\n",
      "Batch 72/165 - Loss: 0.0122\n",
      "Batch 73/165 - Loss: 0.0072\n",
      "Batch 74/165 - Loss: 0.0237\n",
      "Batch 75/165 - Loss: 0.0125\n",
      "Batch 76/165 - Loss: 0.0048\n",
      "Batch 77/165 - Loss: 0.0014\n",
      "Batch 78/165 - Loss: 0.0029\n",
      "Batch 79/165 - Loss: 0.0100\n",
      "Batch 80/165 - Loss: 0.0978\n",
      "Batch 81/165 - Loss: 0.0016\n",
      "Batch 82/165 - Loss: 0.0122\n",
      "Batch 83/165 - Loss: 0.0349\n",
      "Batch 84/165 - Loss: 0.0076\n",
      "Batch 85/165 - Loss: 0.0148\n",
      "Batch 86/165 - Loss: 0.0140\n",
      "Batch 87/165 - Loss: 0.0291\n",
      "Batch 88/165 - Loss: 0.0026\n",
      "Batch 89/165 - Loss: 0.1333\n",
      "Batch 90/165 - Loss: 0.0475\n",
      "Batch 91/165 - Loss: 0.0318\n",
      "Batch 92/165 - Loss: 0.0059\n",
      "Batch 93/165 - Loss: 0.0012\n",
      "Batch 94/165 - Loss: 0.1062\n",
      "Batch 95/165 - Loss: 0.0070\n",
      "Batch 96/165 - Loss: 0.2627\n",
      "Batch 97/165 - Loss: 0.0071\n",
      "Batch 98/165 - Loss: 0.0095\n",
      "Batch 99/165 - Loss: 0.0230\n",
      "Batch 100/165 - Loss: 0.0038\n",
      "Batch 101/165 - Loss: 0.0016\n",
      "Batch 102/165 - Loss: 0.0047\n",
      "Batch 103/165 - Loss: 0.0064\n",
      "Batch 104/165 - Loss: 0.0086\n",
      "Batch 105/165 - Loss: 0.0526\n",
      "Batch 106/165 - Loss: 0.0014\n",
      "Batch 107/165 - Loss: 0.0064\n",
      "Batch 108/165 - Loss: 0.0585\n",
      "Batch 109/165 - Loss: 0.0176\n",
      "Batch 110/165 - Loss: 0.0061\n",
      "Batch 111/165 - Loss: 0.0018\n",
      "Batch 112/165 - Loss: 0.0462\n",
      "Batch 113/165 - Loss: 0.2756\n",
      "Batch 114/165 - Loss: 0.0984\n",
      "Batch 115/165 - Loss: 0.0145\n",
      "Batch 116/165 - Loss: 0.3433\n",
      "Batch 117/165 - Loss: 0.0015\n",
      "Batch 118/165 - Loss: 0.0047\n",
      "Batch 119/165 - Loss: 0.0090\n",
      "Batch 120/165 - Loss: 0.0289\n",
      "Batch 121/165 - Loss: 0.0122\n",
      "Batch 122/165 - Loss: 0.0201\n",
      "Batch 123/165 - Loss: 0.0052\n",
      "Batch 124/165 - Loss: 0.0089\n",
      "Batch 125/165 - Loss: 0.0244\n",
      "Batch 126/165 - Loss: 0.0098\n",
      "Batch 127/165 - Loss: 0.0599\n",
      "Batch 128/165 - Loss: 0.0137\n",
      "Batch 129/165 - Loss: 0.0083\n",
      "Batch 130/165 - Loss: 0.1047\n",
      "Batch 131/165 - Loss: 0.0058\n",
      "Batch 132/165 - Loss: 0.0311\n",
      "Batch 133/165 - Loss: 0.0224\n",
      "Batch 134/165 - Loss: 0.0248\n",
      "Batch 135/165 - Loss: 0.0025\n",
      "Batch 136/165 - Loss: 0.0069\n",
      "Batch 137/165 - Loss: 0.0528\n",
      "Batch 138/165 - Loss: 0.0522\n",
      "Batch 139/165 - Loss: 0.0037\n",
      "Batch 140/165 - Loss: 0.1395\n",
      "Batch 141/165 - Loss: 0.0095\n",
      "Batch 142/165 - Loss: 0.0022\n",
      "Batch 143/165 - Loss: 0.0204\n",
      "Batch 144/165 - Loss: 0.0035\n",
      "Batch 145/165 - Loss: 0.0452\n",
      "Batch 146/165 - Loss: 0.0127\n",
      "Batch 147/165 - Loss: 0.0205\n",
      "Batch 148/165 - Loss: 0.0774\n",
      "Batch 149/165 - Loss: 0.0037\n",
      "Batch 150/165 - Loss: 0.0659\n",
      "Batch 151/165 - Loss: 0.0223\n",
      "Batch 152/165 - Loss: 0.0146\n",
      "Batch 153/165 - Loss: 0.0111\n",
      "Batch 154/165 - Loss: 0.1078\n",
      "Batch 155/165 - Loss: 0.0039\n",
      "Batch 156/165 - Loss: 0.0166\n",
      "Batch 157/165 - Loss: 0.0174\n",
      "Batch 158/165 - Loss: 0.0552\n",
      "Batch 159/165 - Loss: 0.0034\n",
      "Batch 160/165 - Loss: 0.0207\n",
      "Batch 161/165 - Loss: 0.0026\n",
      "Batch 162/165 - Loss: 0.0174\n",
      "Batch 163/165 - Loss: 0.0100\n",
      "Batch 164/165 - Loss: 0.0100\n",
      "Batch 165/165 - Loss: 0.0268\n",
      "Epoch 11/15 | Train Acc: 0.986 | Val Acc: 0.992\n",
      "Batch 1/165 - Loss: 0.0164\n",
      "Batch 2/165 - Loss: 0.0018\n",
      "Batch 3/165 - Loss: 0.0037\n",
      "Batch 4/165 - Loss: 0.0069\n",
      "Batch 5/165 - Loss: 0.0275\n",
      "Batch 6/165 - Loss: 0.1571\n",
      "Batch 7/165 - Loss: 0.1955\n",
      "Batch 8/165 - Loss: 0.0048\n",
      "Batch 9/165 - Loss: 0.0086\n",
      "Batch 10/165 - Loss: 0.0011\n",
      "Batch 11/165 - Loss: 0.0024\n",
      "Batch 12/165 - Loss: 0.0306\n",
      "Batch 13/165 - Loss: 0.0015\n",
      "Batch 14/165 - Loss: 0.1423\n",
      "Batch 15/165 - Loss: 0.0018\n",
      "Batch 16/165 - Loss: 0.0018\n",
      "Batch 17/165 - Loss: 0.0175\n",
      "Batch 18/165 - Loss: 0.0039\n",
      "Batch 19/165 - Loss: 0.0030\n",
      "Batch 20/165 - Loss: 0.2363\n",
      "Batch 21/165 - Loss: 0.0030\n",
      "Batch 22/165 - Loss: 0.0117\n",
      "Batch 23/165 - Loss: 0.0324\n",
      "Batch 24/165 - Loss: 0.0015\n",
      "Batch 25/165 - Loss: 0.0073\n",
      "Batch 26/165 - Loss: 0.0012\n",
      "Batch 27/165 - Loss: 0.0116\n",
      "Batch 28/165 - Loss: 0.0082\n",
      "Batch 29/165 - Loss: 0.0023\n",
      "Batch 30/165 - Loss: 0.0117\n",
      "Batch 31/165 - Loss: 0.0020\n",
      "Batch 32/165 - Loss: 0.0013\n",
      "Batch 33/165 - Loss: 0.0221\n",
      "Batch 34/165 - Loss: 0.1184\n",
      "Batch 35/165 - Loss: 0.0250\n",
      "Batch 36/165 - Loss: 0.0190\n",
      "Batch 37/165 - Loss: 0.1411\n",
      "Batch 38/165 - Loss: 0.0021\n",
      "Batch 39/165 - Loss: 0.0483\n",
      "Batch 40/165 - Loss: 0.0105\n",
      "Batch 41/165 - Loss: 0.0473\n",
      "Batch 42/165 - Loss: 0.0041\n",
      "Batch 43/165 - Loss: 0.0222\n",
      "Batch 44/165 - Loss: 0.0287\n",
      "Batch 45/165 - Loss: 0.0875\n",
      "Batch 46/165 - Loss: 0.0033\n",
      "Batch 47/165 - Loss: 0.0117\n",
      "Batch 48/165 - Loss: 0.0306\n",
      "Batch 49/165 - Loss: 0.0076\n",
      "Batch 50/165 - Loss: 0.0445\n",
      "Batch 51/165 - Loss: 0.0053\n",
      "Batch 52/165 - Loss: 0.1939\n",
      "Batch 53/165 - Loss: 0.0231\n",
      "Batch 54/165 - Loss: 0.0186\n",
      "Batch 55/165 - Loss: 0.0164\n",
      "Batch 56/165 - Loss: 0.1133\n",
      "Batch 57/165 - Loss: 0.0602\n",
      "Batch 58/165 - Loss: 0.0458\n",
      "Batch 59/165 - Loss: 0.0080\n",
      "Batch 60/165 - Loss: 0.0166\n",
      "Batch 61/165 - Loss: 0.1765\n",
      "Batch 62/165 - Loss: 0.0077\n",
      "Batch 63/165 - Loss: 0.0240\n",
      "Batch 64/165 - Loss: 0.0279\n",
      "Batch 65/165 - Loss: 0.0018\n",
      "Batch 66/165 - Loss: 0.0285\n",
      "Batch 67/165 - Loss: 0.0176\n",
      "Batch 68/165 - Loss: 0.0095\n",
      "Batch 69/165 - Loss: 0.0016\n",
      "Batch 70/165 - Loss: 0.0052\n",
      "Batch 71/165 - Loss: 0.0530\n",
      "Batch 72/165 - Loss: 0.0114\n",
      "Batch 73/165 - Loss: 0.0069\n",
      "Batch 74/165 - Loss: 0.0290\n",
      "Batch 75/165 - Loss: 0.0108\n",
      "Batch 76/165 - Loss: 0.0049\n",
      "Batch 77/165 - Loss: 0.0014\n",
      "Batch 78/165 - Loss: 0.0028\n",
      "Batch 79/165 - Loss: 0.0107\n",
      "Batch 80/165 - Loss: 0.1025\n",
      "Batch 81/165 - Loss: 0.0016\n",
      "Batch 82/165 - Loss: 0.0124\n",
      "Batch 83/165 - Loss: 0.0298\n",
      "Batch 84/165 - Loss: 0.0071\n",
      "Batch 85/165 - Loss: 0.0149\n",
      "Batch 86/165 - Loss: 0.0130\n",
      "Batch 87/165 - Loss: 0.0331\n",
      "Batch 88/165 - Loss: 0.0030\n",
      "Batch 89/165 - Loss: 0.1261\n",
      "Batch 90/165 - Loss: 0.0469\n",
      "Batch 91/165 - Loss: 0.0314\n",
      "Batch 92/165 - Loss: 0.0062\n",
      "Batch 93/165 - Loss: 0.0012\n",
      "Batch 94/165 - Loss: 0.0968\n",
      "Batch 95/165 - Loss: 0.0071\n",
      "Batch 96/165 - Loss: 0.2379\n",
      "Batch 97/165 - Loss: 0.0075\n",
      "Batch 98/165 - Loss: 0.0089\n",
      "Batch 99/165 - Loss: 0.0231\n",
      "Batch 100/165 - Loss: 0.0037\n",
      "Batch 101/165 - Loss: 0.0017\n",
      "Batch 102/165 - Loss: 0.0049\n",
      "Batch 103/165 - Loss: 0.0064\n",
      "Batch 104/165 - Loss: 0.0088\n",
      "Batch 105/165 - Loss: 0.0484\n",
      "Batch 106/165 - Loss: 0.0012\n",
      "Batch 107/165 - Loss: 0.0059\n",
      "Batch 108/165 - Loss: 0.0617\n",
      "Batch 109/165 - Loss: 0.0149\n",
      "Batch 110/165 - Loss: 0.0061\n",
      "Batch 111/165 - Loss: 0.0019\n",
      "Batch 112/165 - Loss: 0.0452\n",
      "Batch 113/165 - Loss: 0.2587\n",
      "Batch 114/165 - Loss: 0.1009\n",
      "Batch 115/165 - Loss: 0.0109\n",
      "Batch 116/165 - Loss: 0.3468\n",
      "Batch 117/165 - Loss: 0.0014\n",
      "Batch 118/165 - Loss: 0.0044\n",
      "Batch 119/165 - Loss: 0.0089\n",
      "Batch 120/165 - Loss: 0.0266\n",
      "Batch 121/165 - Loss: 0.0109\n",
      "Batch 122/165 - Loss: 0.0191\n",
      "Batch 123/165 - Loss: 0.0049\n",
      "Batch 124/165 - Loss: 0.0112\n",
      "Batch 125/165 - Loss: 0.0228\n",
      "Batch 126/165 - Loss: 0.0096\n",
      "Batch 127/165 - Loss: 0.0523\n",
      "Batch 128/165 - Loss: 0.0129\n",
      "Batch 129/165 - Loss: 0.0081\n",
      "Batch 130/165 - Loss: 0.0960\n",
      "Batch 131/165 - Loss: 0.0058\n",
      "Batch 132/165 - Loss: 0.0304\n",
      "Batch 133/165 - Loss: 0.0224\n",
      "Batch 134/165 - Loss: 0.0250\n",
      "Batch 135/165 - Loss: 0.0024\n",
      "Batch 136/165 - Loss: 0.0069\n",
      "Batch 137/165 - Loss: 0.0523\n",
      "Batch 138/165 - Loss: 0.0476\n",
      "Batch 139/165 - Loss: 0.0036\n",
      "Batch 140/165 - Loss: 0.1407\n",
      "Batch 141/165 - Loss: 0.0104\n",
      "Batch 142/165 - Loss: 0.0021\n",
      "Batch 143/165 - Loss: 0.0201\n",
      "Batch 144/165 - Loss: 0.0034\n",
      "Batch 145/165 - Loss: 0.0409\n",
      "Batch 146/165 - Loss: 0.0128\n",
      "Batch 147/165 - Loss: 0.0182\n",
      "Batch 148/165 - Loss: 0.0787\n",
      "Batch 149/165 - Loss: 0.0036\n",
      "Batch 150/165 - Loss: 0.0655\n",
      "Batch 151/165 - Loss: 0.0213\n",
      "Batch 152/165 - Loss: 0.0140\n",
      "Batch 153/165 - Loss: 0.0108\n",
      "Batch 154/165 - Loss: 0.1084\n",
      "Batch 155/165 - Loss: 0.0038\n",
      "Batch 156/165 - Loss: 0.0167\n",
      "Batch 157/165 - Loss: 0.0181\n",
      "Batch 158/165 - Loss: 0.0554\n",
      "Batch 159/165 - Loss: 0.0034\n",
      "Batch 160/165 - Loss: 0.0208\n",
      "Batch 161/165 - Loss: 0.0024\n",
      "Batch 162/165 - Loss: 0.0178\n",
      "Batch 163/165 - Loss: 0.0100\n",
      "Batch 164/165 - Loss: 0.0105\n",
      "Batch 165/165 - Loss: 0.0272\n",
      "Epoch 12/15 | Train Acc: 0.986 | Val Acc: 0.992\n",
      "Batch 1/165 - Loss: 0.0171\n",
      "Batch 2/165 - Loss: 0.0017\n",
      "Batch 3/165 - Loss: 0.0037\n",
      "Batch 4/165 - Loss: 0.0071\n",
      "Batch 5/165 - Loss: 0.0268\n",
      "Batch 6/165 - Loss: 0.1731\n",
      "Batch 7/165 - Loss: 0.1928\n",
      "Batch 8/165 - Loss: 0.0048\n",
      "Batch 9/165 - Loss: 0.0093\n",
      "Batch 10/165 - Loss: 0.0010\n",
      "Batch 11/165 - Loss: 0.0024\n",
      "Batch 12/165 - Loss: 0.0308\n",
      "Batch 13/165 - Loss: 0.0014\n",
      "Batch 14/165 - Loss: 0.1418\n",
      "Batch 15/165 - Loss: 0.0017\n",
      "Batch 16/165 - Loss: 0.0016\n",
      "Batch 17/165 - Loss: 0.0172\n",
      "Batch 18/165 - Loss: 0.0038\n",
      "Batch 19/165 - Loss: 0.0026\n",
      "Batch 20/165 - Loss: 0.2329\n",
      "Batch 21/165 - Loss: 0.0030\n",
      "Batch 22/165 - Loss: 0.0105\n",
      "Batch 23/165 - Loss: 0.0319\n",
      "Batch 24/165 - Loss: 0.0015\n",
      "Batch 25/165 - Loss: 0.0075\n",
      "Batch 26/165 - Loss: 0.0011\n",
      "Batch 27/165 - Loss: 0.0121\n",
      "Batch 28/165 - Loss: 0.0083\n",
      "Batch 29/165 - Loss: 0.0024\n",
      "Batch 30/165 - Loss: 0.0117\n",
      "Batch 31/165 - Loss: 0.0020\n",
      "Batch 32/165 - Loss: 0.0013\n",
      "Batch 33/165 - Loss: 0.0224\n",
      "Batch 34/165 - Loss: 0.1145\n",
      "Batch 35/165 - Loss: 0.0250\n",
      "Batch 36/165 - Loss: 0.0214\n",
      "Batch 37/165 - Loss: 0.1529\n",
      "Batch 38/165 - Loss: 0.0021\n",
      "Batch 39/165 - Loss: 0.0473\n",
      "Batch 40/165 - Loss: 0.0104\n",
      "Batch 41/165 - Loss: 0.0471\n",
      "Batch 42/165 - Loss: 0.0044\n",
      "Batch 43/165 - Loss: 0.0220\n",
      "Batch 44/165 - Loss: 0.0296\n",
      "Batch 45/165 - Loss: 0.0856\n",
      "Batch 46/165 - Loss: 0.0033\n",
      "Batch 47/165 - Loss: 0.0121\n",
      "Batch 48/165 - Loss: 0.0309\n",
      "Batch 49/165 - Loss: 0.0073\n",
      "Batch 50/165 - Loss: 0.0599\n",
      "Batch 51/165 - Loss: 0.0071\n",
      "Batch 52/165 - Loss: 0.2118\n",
      "Batch 53/165 - Loss: 0.0428\n",
      "Batch 54/165 - Loss: 0.0218\n",
      "Batch 55/165 - Loss: 0.0164\n",
      "Batch 56/165 - Loss: 0.1111\n",
      "Batch 57/165 - Loss: 0.0566\n",
      "Batch 58/165 - Loss: 0.0464\n",
      "Batch 59/165 - Loss: 0.0087\n",
      "Batch 60/165 - Loss: 0.0168\n",
      "Batch 61/165 - Loss: 0.1701\n",
      "Batch 62/165 - Loss: 0.0081\n",
      "Batch 63/165 - Loss: 0.0246\n",
      "Batch 64/165 - Loss: 0.0336\n",
      "Batch 65/165 - Loss: 0.0032\n",
      "Batch 66/165 - Loss: 0.0275\n",
      "Batch 67/165 - Loss: 0.0197\n",
      "Batch 68/165 - Loss: 0.0093\n",
      "Batch 69/165 - Loss: 0.0025\n",
      "Batch 70/165 - Loss: 0.0051\n",
      "Batch 71/165 - Loss: 0.0559\n",
      "Batch 72/165 - Loss: 0.0142\n",
      "Batch 73/165 - Loss: 0.0069\n",
      "Batch 74/165 - Loss: 0.0340\n",
      "Batch 75/165 - Loss: 0.0158\n",
      "Batch 76/165 - Loss: 0.0053\n",
      "Batch 77/165 - Loss: 0.0013\n",
      "Batch 78/165 - Loss: 0.0036\n",
      "Batch 79/165 - Loss: 0.0134\n",
      "Batch 80/165 - Loss: 0.0905\n",
      "Batch 81/165 - Loss: 0.0015\n",
      "Batch 82/165 - Loss: 0.0129\n",
      "Batch 83/165 - Loss: 0.0341\n",
      "Batch 84/165 - Loss: 0.0069\n",
      "Batch 85/165 - Loss: 0.0151\n",
      "Batch 86/165 - Loss: 0.0138\n",
      "Batch 87/165 - Loss: 0.0292\n",
      "Batch 88/165 - Loss: 0.0024\n",
      "Batch 89/165 - Loss: 0.1417\n",
      "Batch 90/165 - Loss: 0.0425\n",
      "Batch 91/165 - Loss: 0.0301\n",
      "Batch 92/165 - Loss: 0.0049\n",
      "Batch 93/165 - Loss: 0.0013\n",
      "Batch 94/165 - Loss: 0.1074\n",
      "Batch 95/165 - Loss: 0.0059\n",
      "Batch 96/165 - Loss: 0.3126\n",
      "Batch 97/165 - Loss: 0.0073\n",
      "Batch 98/165 - Loss: 0.0106\n",
      "Batch 99/165 - Loss: 0.0203\n",
      "Batch 100/165 - Loss: 0.0035\n",
      "Batch 101/165 - Loss: 0.0041\n",
      "Batch 102/165 - Loss: 0.0078\n",
      "Batch 103/165 - Loss: 0.0342\n",
      "Batch 104/165 - Loss: 0.0090\n",
      "Batch 105/165 - Loss: 0.0535\n",
      "Batch 106/165 - Loss: 0.0067\n",
      "Batch 107/165 - Loss: 0.0160\n",
      "Batch 108/165 - Loss: 0.0636\n",
      "Batch 109/165 - Loss: 0.0264\n",
      "Batch 110/165 - Loss: 0.0075\n",
      "Batch 111/165 - Loss: 0.0033\n",
      "Batch 112/165 - Loss: 0.0502\n",
      "Batch 113/165 - Loss: 0.3070\n",
      "Batch 114/165 - Loss: 0.1072\n",
      "Batch 115/165 - Loss: 0.0165\n",
      "Batch 116/165 - Loss: 0.3487\n",
      "Batch 117/165 - Loss: 0.0022\n",
      "Batch 118/165 - Loss: 0.0054\n",
      "Batch 119/165 - Loss: 0.0106\n",
      "Batch 120/165 - Loss: 0.0205\n",
      "Batch 121/165 - Loss: 0.0130\n",
      "Batch 122/165 - Loss: 0.0205\n",
      "Batch 123/165 - Loss: 0.0070\n",
      "Batch 124/165 - Loss: 0.0082\n",
      "Batch 125/165 - Loss: 0.0212\n",
      "Batch 126/165 - Loss: 0.0115\n",
      "Batch 127/165 - Loss: 0.0839\n",
      "Batch 128/165 - Loss: 0.0108\n",
      "Batch 129/165 - Loss: 0.0089\n",
      "Batch 130/165 - Loss: 0.1093\n",
      "Batch 131/165 - Loss: 0.0061\n",
      "Batch 132/165 - Loss: 0.0358\n",
      "Batch 133/165 - Loss: 0.0214\n",
      "Batch 134/165 - Loss: 0.0335\n",
      "Batch 135/165 - Loss: 0.0037\n",
      "Batch 136/165 - Loss: 0.0064\n",
      "Batch 137/165 - Loss: 0.0508\n",
      "Batch 138/165 - Loss: 0.0549\n",
      "Batch 139/165 - Loss: 0.0047\n",
      "Batch 140/165 - Loss: 0.1598\n",
      "Batch 141/165 - Loss: 0.0101\n",
      "Batch 142/165 - Loss: 0.0028\n",
      "Batch 143/165 - Loss: 0.0166\n",
      "Batch 144/165 - Loss: 0.0029\n",
      "Batch 145/165 - Loss: 0.0462\n",
      "Batch 146/165 - Loss: 0.0113\n",
      "Batch 147/165 - Loss: 0.0212\n",
      "Batch 148/165 - Loss: 0.0855\n",
      "Batch 149/165 - Loss: 0.0036\n",
      "Batch 150/165 - Loss: 0.0492\n",
      "Batch 151/165 - Loss: 0.0202\n",
      "Batch 152/165 - Loss: 0.0143\n",
      "Batch 153/165 - Loss: 0.0111\n",
      "Batch 154/165 - Loss: 0.1119\n",
      "Batch 155/165 - Loss: 0.0038\n",
      "Batch 156/165 - Loss: 0.0175\n",
      "Batch 157/165 - Loss: 0.0212\n",
      "Batch 158/165 - Loss: 0.0603\n",
      "Batch 159/165 - Loss: 0.0030\n",
      "Batch 160/165 - Loss: 0.0211\n",
      "Batch 161/165 - Loss: 0.0029\n",
      "Batch 162/165 - Loss: 0.0182\n",
      "Batch 163/165 - Loss: 0.0103\n",
      "Batch 164/165 - Loss: 0.0137\n",
      "Batch 165/165 - Loss: 0.0350\n",
      "Epoch 13/15 | Train Acc: 0.985 | Val Acc: 0.997\n",
      "Batch 1/165 - Loss: 0.0178\n",
      "Batch 2/165 - Loss: 0.0015\n",
      "Batch 3/165 - Loss: 0.0041\n",
      "Batch 4/165 - Loss: 0.0063\n",
      "Batch 5/165 - Loss: 0.0281\n",
      "Batch 6/165 - Loss: 0.1138\n",
      "Batch 7/165 - Loss: 0.1983\n",
      "Batch 8/165 - Loss: 0.0057\n",
      "Batch 9/165 - Loss: 0.0090\n",
      "Batch 10/165 - Loss: 0.0011\n",
      "Batch 11/165 - Loss: 0.0031\n",
      "Batch 12/165 - Loss: 0.0358\n",
      "Batch 13/165 - Loss: 0.0019\n",
      "Batch 14/165 - Loss: 0.1499\n",
      "Batch 15/165 - Loss: 0.0020\n",
      "Batch 16/165 - Loss: 0.0014\n",
      "Batch 17/165 - Loss: 0.0145\n",
      "Batch 18/165 - Loss: 0.0039\n",
      "Batch 19/165 - Loss: 0.0023\n",
      "Batch 20/165 - Loss: 0.2277\n",
      "Batch 21/165 - Loss: 0.0030\n",
      "Batch 22/165 - Loss: 0.0099\n",
      "Batch 23/165 - Loss: 0.0318\n",
      "Batch 24/165 - Loss: 0.0012\n",
      "Batch 25/165 - Loss: 0.0086\n",
      "Batch 26/165 - Loss: 0.0012\n",
      "Batch 27/165 - Loss: 0.0094\n",
      "Batch 28/165 - Loss: 0.0084\n",
      "Batch 29/165 - Loss: 0.0030\n",
      "Batch 30/165 - Loss: 0.0107\n",
      "Batch 31/165 - Loss: 0.0021\n",
      "Batch 32/165 - Loss: 0.0010\n",
      "Batch 33/165 - Loss: 0.0245\n",
      "Batch 34/165 - Loss: 0.1029\n",
      "Batch 35/165 - Loss: 0.0248\n",
      "Batch 36/165 - Loss: 0.0225\n",
      "Batch 37/165 - Loss: 0.1374\n",
      "Batch 38/165 - Loss: 0.0016\n",
      "Batch 39/165 - Loss: 0.0503\n",
      "Batch 40/165 - Loss: 0.0102\n",
      "Batch 41/165 - Loss: 0.0433\n",
      "Batch 42/165 - Loss: 0.0041\n",
      "Batch 43/165 - Loss: 0.0234\n",
      "Batch 44/165 - Loss: 0.0292\n",
      "Batch 45/165 - Loss: 0.0828\n",
      "Batch 46/165 - Loss: 0.0030\n",
      "Batch 47/165 - Loss: 0.0112\n",
      "Batch 48/165 - Loss: 0.0315\n",
      "Batch 49/165 - Loss: 0.0060\n",
      "Batch 50/165 - Loss: 0.0409\n",
      "Batch 51/165 - Loss: 0.0033\n",
      "Batch 52/165 - Loss: 0.1771\n",
      "Batch 53/165 - Loss: 0.0211\n",
      "Batch 54/165 - Loss: 0.0106\n",
      "Batch 55/165 - Loss: 0.0147\n",
      "Batch 56/165 - Loss: 0.0979\n",
      "Batch 57/165 - Loss: 0.0646\n",
      "Batch 58/165 - Loss: 0.0452\n",
      "Batch 59/165 - Loss: 0.0077\n",
      "Batch 60/165 - Loss: 0.0155\n",
      "Batch 61/165 - Loss: 0.1780\n",
      "Batch 62/165 - Loss: 0.0077\n",
      "Batch 63/165 - Loss: 0.0233\n",
      "Batch 64/165 - Loss: 0.0340\n",
      "Batch 65/165 - Loss: 0.0015\n",
      "Batch 66/165 - Loss: 0.0260\n",
      "Batch 67/165 - Loss: 0.0155\n",
      "Batch 68/165 - Loss: 0.0086\n",
      "Batch 69/165 - Loss: 0.0016\n",
      "Batch 70/165 - Loss: 0.0067\n",
      "Batch 71/165 - Loss: 0.0548\n",
      "Batch 72/165 - Loss: 0.0109\n",
      "Batch 73/165 - Loss: 0.0072\n",
      "Batch 74/165 - Loss: 0.0282\n",
      "Batch 75/165 - Loss: 0.0136\n",
      "Batch 76/165 - Loss: 0.0048\n",
      "Batch 77/165 - Loss: 0.0012\n",
      "Batch 78/165 - Loss: 0.0031\n",
      "Batch 79/165 - Loss: 0.0115\n",
      "Batch 80/165 - Loss: 0.0919\n",
      "Batch 81/165 - Loss: 0.0015\n",
      "Batch 82/165 - Loss: 0.0124\n",
      "Batch 83/165 - Loss: 0.0336\n",
      "Batch 84/165 - Loss: 0.0068\n",
      "Batch 85/165 - Loss: 0.0144\n",
      "Batch 86/165 - Loss: 0.0143\n",
      "Batch 87/165 - Loss: 0.0294\n",
      "Batch 88/165 - Loss: 0.0028\n",
      "Batch 89/165 - Loss: 0.1349\n",
      "Batch 90/165 - Loss: 0.0429\n",
      "Batch 91/165 - Loss: 0.0314\n",
      "Batch 92/165 - Loss: 0.0050\n",
      "Batch 93/165 - Loss: 0.0011\n",
      "Batch 94/165 - Loss: 0.0957\n",
      "Batch 95/165 - Loss: 0.0055\n",
      "Batch 96/165 - Loss: 0.2927\n",
      "Batch 97/165 - Loss: 0.0071\n",
      "Batch 98/165 - Loss: 0.0092\n",
      "Batch 99/165 - Loss: 0.0216\n",
      "Batch 100/165 - Loss: 0.0030\n",
      "Batch 101/165 - Loss: 0.0016\n",
      "Batch 102/165 - Loss: 0.0056\n",
      "Batch 103/165 - Loss: 0.0081\n",
      "Batch 104/165 - Loss: 0.0091\n",
      "Batch 105/165 - Loss: 0.0396\n",
      "Batch 106/165 - Loss: 0.0010\n",
      "Batch 107/165 - Loss: 0.0061\n",
      "Batch 108/165 - Loss: 0.0701\n",
      "Batch 109/165 - Loss: 0.0137\n",
      "Batch 110/165 - Loss: 0.0064\n",
      "Batch 111/165 - Loss: 0.0020\n",
      "Batch 112/165 - Loss: 0.0423\n",
      "Batch 113/165 - Loss: 0.2782\n",
      "Batch 114/165 - Loss: 0.1009\n",
      "Batch 115/165 - Loss: 0.0182\n",
      "Batch 116/165 - Loss: 0.3505\n",
      "Batch 117/165 - Loss: 0.0012\n",
      "Batch 118/165 - Loss: 0.0036\n",
      "Batch 119/165 - Loss: 0.0070\n",
      "Batch 120/165 - Loss: 0.0213\n",
      "Batch 121/165 - Loss: 0.0110\n",
      "Batch 122/165 - Loss: 0.0164\n",
      "Batch 123/165 - Loss: 0.0058\n",
      "Batch 124/165 - Loss: 0.0131\n",
      "Batch 125/165 - Loss: 0.0188\n",
      "Batch 126/165 - Loss: 0.0096\n",
      "Batch 127/165 - Loss: 0.0443\n",
      "Batch 128/165 - Loss: 0.0106\n",
      "Batch 129/165 - Loss: 0.0074\n",
      "Batch 130/165 - Loss: 0.0907\n",
      "Batch 131/165 - Loss: 0.0058\n",
      "Batch 132/165 - Loss: 0.0271\n",
      "Batch 133/165 - Loss: 0.0213\n",
      "Batch 134/165 - Loss: 0.0267\n",
      "Batch 135/165 - Loss: 0.0025\n",
      "Batch 136/165 - Loss: 0.0062\n",
      "Batch 137/165 - Loss: 0.0488\n",
      "Batch 138/165 - Loss: 0.0479\n",
      "Batch 139/165 - Loss: 0.0040\n",
      "Batch 140/165 - Loss: 0.1567\n",
      "Batch 141/165 - Loss: 0.0114\n",
      "Batch 142/165 - Loss: 0.0022\n",
      "Batch 143/165 - Loss: 0.0161\n",
      "Batch 144/165 - Loss: 0.0041\n",
      "Batch 145/165 - Loss: 0.0330\n",
      "Batch 146/165 - Loss: 0.0135\n",
      "Batch 147/165 - Loss: 0.0141\n",
      "Batch 148/165 - Loss: 0.0813\n",
      "Batch 149/165 - Loss: 0.0034\n",
      "Batch 150/165 - Loss: 0.0605\n",
      "Batch 151/165 - Loss: 0.0218\n",
      "Batch 152/165 - Loss: 0.0152\n",
      "Batch 153/165 - Loss: 0.0115\n",
      "Batch 154/165 - Loss: 0.1096\n",
      "Batch 155/165 - Loss: 0.0034\n",
      "Batch 156/165 - Loss: 0.0171\n",
      "Batch 157/165 - Loss: 0.0188\n",
      "Batch 158/165 - Loss: 0.0585\n",
      "Batch 159/165 - Loss: 0.0031\n",
      "Batch 160/165 - Loss: 0.0217\n",
      "Batch 161/165 - Loss: 0.0023\n",
      "Batch 162/165 - Loss: 0.0169\n",
      "Batch 163/165 - Loss: 0.0094\n",
      "Batch 164/165 - Loss: 0.0130\n",
      "Batch 165/165 - Loss: 0.0339\n",
      "Epoch 14/15 | Train Acc: 0.986 | Val Acc: 0.999\n",
      "Batch 1/165 - Loss: 0.0183\n",
      "Batch 2/165 - Loss: 0.0009\n",
      "Batch 3/165 - Loss: 0.0042\n",
      "Batch 4/165 - Loss: 0.0105\n",
      "Batch 5/165 - Loss: 0.0285\n",
      "Batch 6/165 - Loss: 0.1419\n",
      "Batch 7/165 - Loss: 0.1828\n",
      "Batch 8/165 - Loss: 0.0057\n",
      "Batch 9/165 - Loss: 0.0098\n",
      "Batch 10/165 - Loss: 0.0009\n",
      "Batch 11/165 - Loss: 0.0026\n",
      "Batch 12/165 - Loss: 0.0333\n",
      "Batch 13/165 - Loss: 0.0016\n",
      "Batch 14/165 - Loss: 0.1508\n",
      "Batch 15/165 - Loss: 0.0019\n",
      "Batch 16/165 - Loss: 0.0015\n",
      "Batch 17/165 - Loss: 0.0152\n",
      "Batch 18/165 - Loss: 0.0037\n",
      "Batch 19/165 - Loss: 0.0024\n",
      "Batch 20/165 - Loss: 0.2187\n",
      "Batch 21/165 - Loss: 0.0028\n",
      "Batch 22/165 - Loss: 0.0106\n",
      "Batch 23/165 - Loss: 0.0296\n",
      "Batch 24/165 - Loss: 0.0013\n",
      "Batch 25/165 - Loss: 0.0080\n",
      "Batch 26/165 - Loss: 0.0011\n",
      "Batch 27/165 - Loss: 0.0128\n",
      "Batch 28/165 - Loss: 0.0079\n",
      "Batch 29/165 - Loss: 0.0029\n",
      "Batch 30/165 - Loss: 0.0114\n",
      "Batch 31/165 - Loss: 0.0020\n",
      "Batch 32/165 - Loss: 0.0012\n",
      "Batch 33/165 - Loss: 0.0249\n",
      "Batch 34/165 - Loss: 0.1020\n",
      "Batch 35/165 - Loss: 0.0248\n",
      "Batch 36/165 - Loss: 0.0296\n",
      "Batch 37/165 - Loss: 0.1589\n",
      "Batch 38/165 - Loss: 0.0018\n",
      "Batch 39/165 - Loss: 0.0492\n",
      "Batch 40/165 - Loss: 0.0097\n",
      "Batch 41/165 - Loss: 0.0418\n",
      "Batch 42/165 - Loss: 0.0039\n",
      "Batch 43/165 - Loss: 0.0235\n",
      "Batch 44/165 - Loss: 0.0311\n",
      "Batch 45/165 - Loss: 0.0794\n",
      "Batch 46/165 - Loss: 0.0032\n",
      "Batch 47/165 - Loss: 0.0122\n",
      "Batch 48/165 - Loss: 0.0330\n",
      "Batch 49/165 - Loss: 0.0061\n",
      "Batch 50/165 - Loss: 0.0378\n",
      "Batch 51/165 - Loss: 0.0039\n",
      "Batch 52/165 - Loss: 0.1941\n",
      "Batch 53/165 - Loss: 0.0277\n",
      "Batch 54/165 - Loss: 0.0134\n",
      "Batch 55/165 - Loss: 0.0148\n",
      "Batch 56/165 - Loss: 0.1182\n",
      "Batch 57/165 - Loss: 0.0637\n",
      "Batch 58/165 - Loss: 0.0432\n",
      "Batch 59/165 - Loss: 0.0075\n",
      "Batch 60/165 - Loss: 0.0159\n",
      "Batch 61/165 - Loss: 0.1874\n",
      "Batch 62/165 - Loss: 0.0075\n",
      "Batch 63/165 - Loss: 0.0211\n",
      "Batch 64/165 - Loss: 0.0209\n",
      "Batch 65/165 - Loss: 0.0013\n",
      "Batch 66/165 - Loss: 0.0250\n",
      "Batch 67/165 - Loss: 0.0141\n",
      "Batch 68/165 - Loss: 0.0085\n",
      "Batch 69/165 - Loss: 0.0012\n",
      "Batch 70/165 - Loss: 0.0059\n",
      "Batch 71/165 - Loss: 0.0378\n",
      "Batch 72/165 - Loss: 0.0092\n",
      "Batch 73/165 - Loss: 0.0069\n",
      "Batch 74/165 - Loss: 0.0261\n",
      "Batch 75/165 - Loss: 0.0100\n",
      "Batch 76/165 - Loss: 0.0045\n",
      "Batch 77/165 - Loss: 0.0011\n",
      "Batch 78/165 - Loss: 0.0031\n",
      "Batch 79/165 - Loss: 0.0108\n",
      "Batch 80/165 - Loss: 0.0927\n",
      "Batch 81/165 - Loss: 0.0013\n",
      "Batch 82/165 - Loss: 0.0130\n",
      "Batch 83/165 - Loss: 0.0339\n",
      "Batch 84/165 - Loss: 0.0063\n",
      "Batch 85/165 - Loss: 0.0154\n",
      "Batch 86/165 - Loss: 0.0144\n",
      "Batch 87/165 - Loss: 0.0298\n",
      "Batch 88/165 - Loss: 0.0028\n",
      "Batch 89/165 - Loss: 0.1343\n",
      "Batch 90/165 - Loss: 0.0430\n",
      "Batch 91/165 - Loss: 0.0311\n",
      "Batch 92/165 - Loss: 0.0048\n",
      "Batch 93/165 - Loss: 0.0009\n",
      "Batch 94/165 - Loss: 0.0963\n",
      "Batch 95/165 - Loss: 0.0050\n",
      "Batch 96/165 - Loss: 0.3227\n",
      "Batch 97/165 - Loss: 0.0068\n",
      "Batch 98/165 - Loss: 0.0092\n",
      "Batch 99/165 - Loss: 0.0206\n",
      "Batch 100/165 - Loss: 0.0030\n",
      "Batch 101/165 - Loss: 0.0022\n",
      "Batch 102/165 - Loss: 0.0065\n",
      "Batch 103/165 - Loss: 0.0222\n",
      "Batch 104/165 - Loss: 0.0098\n",
      "Batch 105/165 - Loss: 0.0401\n",
      "Batch 106/165 - Loss: 0.0024\n",
      "Batch 107/165 - Loss: 0.0105\n",
      "Batch 108/165 - Loss: 0.0773\n",
      "Batch 109/165 - Loss: 0.0199\n",
      "Batch 110/165 - Loss: 0.0069\n",
      "Batch 111/165 - Loss: 0.0025\n",
      "Batch 112/165 - Loss: 0.0438\n",
      "Batch 113/165 - Loss: 0.3081\n",
      "Batch 114/165 - Loss: 0.1046\n",
      "Batch 115/165 - Loss: 0.0193\n",
      "Batch 116/165 - Loss: 0.3631\n",
      "Batch 117/165 - Loss: 0.0014\n",
      "Batch 118/165 - Loss: 0.0036\n",
      "Batch 119/165 - Loss: 0.0070\n",
      "Batch 120/165 - Loss: 0.0176\n",
      "Batch 121/165 - Loss: 0.0116\n",
      "Batch 122/165 - Loss: 0.0169\n",
      "Batch 123/165 - Loss: 0.0067\n",
      "Batch 124/165 - Loss: 0.0091\n",
      "Batch 125/165 - Loss: 0.0175\n",
      "Batch 126/165 - Loss: 0.0110\n",
      "Batch 127/165 - Loss: 0.0579\n",
      "Batch 128/165 - Loss: 0.0095\n",
      "Batch 129/165 - Loss: 0.0077\n",
      "Batch 130/165 - Loss: 0.0992\n",
      "Batch 131/165 - Loss: 0.0061\n",
      "Batch 132/165 - Loss: 0.0310\n",
      "Batch 133/165 - Loss: 0.0208\n",
      "Batch 134/165 - Loss: 0.0322\n",
      "Batch 135/165 - Loss: 0.0036\n",
      "Batch 136/165 - Loss: 0.0061\n",
      "Batch 137/165 - Loss: 0.0455\n",
      "Batch 138/165 - Loss: 0.0441\n",
      "Batch 139/165 - Loss: 0.0049\n",
      "Batch 140/165 - Loss: 0.1663\n",
      "Batch 141/165 - Loss: 0.0129\n",
      "Batch 142/165 - Loss: 0.0027\n",
      "Batch 143/165 - Loss: 0.0140\n",
      "Batch 144/165 - Loss: 0.0050\n",
      "Batch 145/165 - Loss: 0.0322\n",
      "Batch 146/165 - Loss: 0.0149\n",
      "Batch 147/165 - Loss: 0.0155\n",
      "Batch 148/165 - Loss: 0.0807\n",
      "Batch 149/165 - Loss: 0.0038\n",
      "Batch 150/165 - Loss: 0.0590\n",
      "Batch 151/165 - Loss: 0.0215\n",
      "Batch 152/165 - Loss: 0.0155\n",
      "Batch 153/165 - Loss: 0.0123\n",
      "Batch 154/165 - Loss: 0.1086\n",
      "Batch 155/165 - Loss: 0.0036\n",
      "Batch 156/165 - Loss: 0.0169\n",
      "Batch 157/165 - Loss: 0.0220\n",
      "Batch 158/165 - Loss: 0.0606\n",
      "Batch 159/165 - Loss: 0.0029\n",
      "Batch 160/165 - Loss: 0.0209\n",
      "Batch 161/165 - Loss: 0.0027\n",
      "Batch 162/165 - Loss: 0.0186\n",
      "Batch 163/165 - Loss: 0.0095\n",
      "Batch 164/165 - Loss: 0.0145\n",
      "Batch 165/165 - Loss: 0.0346\n",
      "Epoch 15/15 | Train Acc: 0.985 | Val Acc: 0.997\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, X_train_t, y_train_t, optimizer, criterion, BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model, X_val_t, y_val_t, criterion\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4d69e",
   "metadata": {},
   "source": [
    "On synthetic data, you should see:\n",
    "\n",
    "- accuracy quickly rise above 90%\n",
    "- validation track training closely\n",
    "\n",
    "That confirms:\n",
    "- dataset is usable\n",
    "- model is learning\n",
    "- pipeline is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ebe67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: torch.Size([2250, 75, 3]) torch.Size([2250])\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "X_test = np.load(\"../data/X_test.npy\")\n",
    "y_test = np.load(\"../data/y_test.npy\")\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"Test shape:\", X_test_t.shape, y_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfcbb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.999\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = evaluate(\n",
    "    model, X_test_t, y_test_t, criterion\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481361e",
   "metadata": {},
   "source": [
    "You should expect:\n",
    "- Test accuracy  validation accuracy\n",
    "- Slight drop is okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e2b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[776   3   0]\n",
      " [  0 711   0]\n",
      " [  0   0 760]]\n",
      "\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    Light Braking       1.00      1.00      1.00       779\n",
      "   Normal Braking       1.00      1.00      1.00       711\n",
      "Emergency Braking       1.00      1.00      1.00       760\n",
      "\n",
      "         accuracy                           1.00      2250\n",
      "        macro avg       1.00      1.00      1.00      2250\n",
      "     weighted avg       1.00      1.00      1.00      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_t)\n",
    "    preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds, target_names=[\n",
    "    \"Light Braking\", \"Normal Braking\", \"Emergency Braking\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f55c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"../models/lstm_cnn_attention_baseline.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45609656",
   "metadata": {},
   "source": [
    "Files after saving this model \n",
    "\n",
    "models/\n",
    "- lstm_cnn_attention.py\n",
    "- lstm_cnn_attention_baseline.pth\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
