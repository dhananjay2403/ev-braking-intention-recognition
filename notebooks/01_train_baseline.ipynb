{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daea0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc05a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from models.lstm_cnn_attention import LSTMCNNAttention\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2cdeb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for LSTM-CNN-Attention model\n",
    "\n",
    "model = LSTMCNNAttention()\n",
    "\n",
    "x = torch.randn(8, 75, 3)  # batch of 8 samples\n",
    "y = model(x)\n",
    "\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98bdd7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 75, 3])\n",
      "Reconstructed shape: torch.Size([4, 75, 3])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check for Autoencoder \n",
    "\n",
    "from models.sequence_autoencoder import SequenceAutoencoder\n",
    "\n",
    "ae = SequenceAutoencoder()\n",
    "\n",
    "x = torch.randn(4, 75, 3)\n",
    "recon = ae(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Reconstructed shape:\", recon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cbaaf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (10500, 75, 3) (10500,)\n",
      "Val shape: (2250, 75, 3) (2250,)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-generated dataset\n",
    "X_train = np.load(\"../data/X_train.npy\")\n",
    "y_train = np.load(\"../data/y_train.npy\")\n",
    "\n",
    "X_val = np.load(\"../data/X_val.npy\")\n",
    "y_val = np.load(\"../data/y_val.npy\")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Val shape:\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "038c9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "'''\n",
    "    Why?\n",
    "    PyTorch models only work with tensors\n",
    "    Labels must be long for classification\n",
    "'''\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype = torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype = torch.long)\n",
    "\n",
    "X_val_t = torch.tensor(X_val, dtype = torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7f7aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = LSTMCNNAttention()\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "# Training params\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c3634",
   "metadata": {},
   "source": [
    "Simple. No tuning yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b320a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, X, y, optimizer, criterion, batch_size):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    num_batches = (len(X) + batch_size - 1) // batch_size\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = X[i:i + batch_size]\n",
    "        yb = y[i:i + batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim = 1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Batch {i//batch_size + 1}/{num_batches} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    acc = correct / len(X)\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94fa7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop\n",
    "\n",
    "def evaluate(model, X, y, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        total_loss = loss.item()\n",
    "        preds = outputs.argmax(dim = 1)\n",
    "        correct = (preds == y).sum().item()\n",
    "\n",
    "    acc = correct / len(X)\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, X_train_t, y_train_t, optimizer, criterion, BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model, X_val_t, y_val_t, criterion\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecba6c1",
   "metadata": {},
   "source": [
    "```\n",
    "Batch 1/165 - Loss: 1.0990\n",
    "Batch 2/165 - Loss: 1.0990\n",
    "Batch 3/165 - Loss: 1.0894\n",
    "Batch 4/165 - Loss: 1.0835\n",
    "Batch 5/165 - Loss: 1.0903\n",
    "...\n",
    "Batch 163/165 - Loss: 0.0109\n",
    "Batch 164/165 - Loss: 0.0107\n",
    "Batch 165/165 - Loss: 0.0208\n",
    "```\n",
    "\n",
    "Epoch 15/15 | Train Acc: 0.986 | Val Acc: 0.997"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4d69e",
   "metadata": {},
   "source": [
    "On synthetic data, you should see:\n",
    "\n",
    "- accuracy quickly rise above 90%\n",
    "- validation track training closely\n",
    "\n",
    "That confirms:\n",
    "- dataset is usable\n",
    "- model is learning\n",
    "- pipeline is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3ebe67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: torch.Size([2250, 75, 3]) torch.Size([2250])\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "X_test = np.load(\"../data/X_test.npy\")\n",
    "y_test = np.load(\"../data/y_test.npy\")\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"Test shape:\", X_test_t.shape, y_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8cfcbb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.998\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc = evaluate(\n",
    "    model, X_test_t, y_test_t, criterion\n",
    ")\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3481361e",
   "metadata": {},
   "source": [
    "You should expect:\n",
    "- Test accuracy ≈ validation accuracy\n",
    "- Slight drop is okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8e2b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[775   4   0]\n",
      " [  0 711   0]\n",
      " [  0   0 760]]\n",
      "\n",
      "Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    Light Braking       1.00      0.99      1.00       779\n",
      "   Normal Braking       0.99      1.00      1.00       711\n",
      "Emergency Braking       1.00      1.00      1.00       760\n",
      "\n",
      "         accuracy                           1.00      2250\n",
      "        macro avg       1.00      1.00      1.00      2250\n",
      "     weighted avg       1.00      1.00      1.00      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_t)\n",
    "    preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, preds, target_names=[\n",
    "    \"Light Braking\", \"Normal Braking\", \"Emergency Braking\"\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f55c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"../models/lstm_cnn_attention_baseline.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45609656",
   "metadata": {},
   "source": [
    "Files after saving this model \n",
    "\n",
    "models/\n",
    "- lstm_cnn_attention.py\n",
    "- lstm_cnn_attention_baseline.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5833a03a",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa5deabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae = SequenceAutoencoder()\n",
    "\n",
    "ae_criterion = nn.MSELoss()\n",
    "ae_optimizer = optim.Adam(ae.parameters(), lr=1e-3)\n",
    "\n",
    "AE_EPOCHS = 20\n",
    "AE_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da91921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Autoencoder \n",
    "def train_autoencoder(model, X, optimizer, criterion, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        xb = X[i:i+batch_size]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon = model(xb)\n",
    "        loss = criterion(recon, xb)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3557f0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE Epoch 1/20 | Reconstruction Loss: 1.573429\n",
      "AE Epoch 2/20 | Reconstruction Loss: 0.075039\n",
      "AE Epoch 3/20 | Reconstruction Loss: 0.025374\n",
      "AE Epoch 4/20 | Reconstruction Loss: 0.017062\n",
      "AE Epoch 5/20 | Reconstruction Loss: 0.004242\n",
      "AE Epoch 6/20 | Reconstruction Loss: 0.000468\n",
      "AE Epoch 7/20 | Reconstruction Loss: 0.000388\n",
      "AE Epoch 8/20 | Reconstruction Loss: 0.000373\n",
      "AE Epoch 9/20 | Reconstruction Loss: 0.000358\n",
      "AE Epoch 10/20 | Reconstruction Loss: 0.000344\n",
      "AE Epoch 11/20 | Reconstruction Loss: 0.000329\n",
      "AE Epoch 12/20 | Reconstruction Loss: 0.000314\n",
      "AE Epoch 13/20 | Reconstruction Loss: 0.000300\n",
      "AE Epoch 14/20 | Reconstruction Loss: 0.000285\n",
      "AE Epoch 15/20 | Reconstruction Loss: 0.000271\n",
      "AE Epoch 16/20 | Reconstruction Loss: 0.000258\n",
      "AE Epoch 17/20 | Reconstruction Loss: 0.000245\n",
      "AE Epoch 18/20 | Reconstruction Loss: 0.000233\n",
      "AE Epoch 19/20 | Reconstruction Loss: 0.000221\n",
      "AE Epoch 20/20 | Reconstruction Loss: 0.000210\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(AE_EPOCHS):\n",
    "    loss = train_autoencoder(\n",
    "        ae, X_train_t, ae_optimizer, ae_criterion, AE_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"AE Epoch {epoch+1}/{AE_EPOCHS} | Reconstruction Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5aa3c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder saved.\n"
     ]
    }
   ],
   "source": [
    "# Save trained Autoencoder \n",
    "torch.save(ae.state_dict(), \"../models/sequence_autoencoder.pth\")\n",
    "\n",
    "print(\"Autoencoder saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8fe8949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# AE + Classifier Sanity check\n",
    "from models.lstm_cnn_attention import AE_LSTMCNNAttention\n",
    "\n",
    "ae_model = AE_LSTMCNNAttention()\n",
    "\n",
    "x = torch.randn(4, 75, 3)\n",
    "y = ae_model(x)\n",
    "\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76b589f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the integrated model\n",
    "ae_classifier = AE_LSTMCNNAttention()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, ae_classifier.parameters()),\n",
    "    lr = 1e-3\n",
    ")\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        ae_classifier, X_train_t, y_train_t,\n",
    "        optimizer, criterion, BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        ae_classifier, X_val_t, y_val_t, criterion\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[AE+CLS] Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78398d47",
   "metadata": {},
   "source": [
    "```\n",
    "Batch 1/165 - Loss: 1.0949\n",
    "Batch 2/165 - Loss: 1.0915\n",
    "Batch 3/165 - Loss: 1.0816\n",
    "Batch 4/165 - Loss: 1.0733\n",
    "Batch 5/165 - Loss: 1.0732\n",
    "...\n",
    "Batch 163/165 - Loss: 0.0027\n",
    "Batch 164/165 - Loss: 0.0032\n",
    "Batch 165/165 - Loss: 0.0004\n",
    "```\n",
    "\n",
    "[AE+CLS] Epoch 15/15 | Train Acc: 0.996 | Val Acc: 0.994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94bf66",
   "metadata": {},
   "source": [
    "## Test-set evaluation for AE + Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040b427e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set shape: torch.Size([2250, 75, 3]) torch.Size([2250])\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "X_test = np.load(\"../data/X_test.npy\")\n",
    "y_test = np.load(\"../data/y_test.npy\")\n",
    "\n",
    "X_test_t = torch.tensor(X_test, dtype = torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype = torch.long)\n",
    "\n",
    "print(\"Test set shape:\", X_test_t.shape, y_test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6dbf02a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AE+CLS] Test Accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(\n",
    "    ae_classifier, X_test_t, y_test_t, criterion\n",
    ")\n",
    "\n",
    "print(f\"[AE+CLS] Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (AE+CLS):\n",
      " [[779   0   0]\n",
      " [ 16 695   0]\n",
      " [  0   0 760]]\n",
      "\n",
      "Classification Report (AE+CLS):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    Light Braking       0.98      1.00      0.99       779\n",
      "   Normal Braking       1.00      0.98      0.99       711\n",
      "Emergency Braking       1.00      1.00      1.00       760\n",
      "\n",
      "         accuracy                           0.99      2250\n",
      "        macro avg       0.99      0.99      0.99      2250\n",
      "     weighted avg       0.99      0.99      0.99      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "ae_classifier.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = ae_classifier(X_test_t)\n",
    "    preds = outputs.argmax(dim = 1).cpu().numpy()\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "print(\"Confusion Matrix (AE+CLS):\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report (AE+CLS):\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    preds,\n",
    "    target_names = [\"Light Braking\", \"Normal Braking\", \"Emergency Braking\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdd4e1",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "Test results:\n",
    "\n",
    "Test Accuracy: 99.29%\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "    Light     → almost perfect\n",
    "    Normal    → small confusion with Light (16 samples)\n",
    "    Emergency → perfect\n",
    "\n",
    "This tells us three important things:\n",
    "\n",
    "✅ (1) No train–test leakage\n",
    "\n",
    "If there was leakage:\n",
    "- test accuracy would be ~100%\n",
    "- confusion matrix would be perfectly diagonal\n",
    "\n",
    "But we do have:\n",
    "- small, realistic confusion (Normal ↔ Light)\n",
    "- slightly lower test accuracy than val\n",
    "\n",
    "This is healthy.\n",
    "\n",
    "✅ (2) Emergency braking is learned robustly\n",
    "\n",
    "This is critical for both real-world relevance & research credibility\n",
    "\n",
    "Emergency braking:\n",
    "- Precision = 1.00\n",
    "- Recall = 1.00\n",
    "\n",
    "This means the model has learned clear temporal patterns for emergency braking, not just thresholds.\n",
    "\n",
    "⚠️ (3) The data distribution is still “easy”\n",
    "\n",
    "Your concern was:\n",
    "\n",
    "“The model has learned the data instead of understanding patterns.”\n",
    "\n",
    "The correct refined statement is:\n",
    "\n",
    "“The model understands patterns very well — but the patterns themselves are still too clean and consistent.”\n",
    "\n",
    "This is a data realism issue, not a model issue. That’s an important distinction.\n",
    "\n",
    "2️⃣ So… is this a problem?\n",
    "❌ No, this is NOT a problem at this stage\n",
    "✅ This is actually the expected outcome\n",
    "\n",
    "Why?\n",
    "- You deliberately started with clean synthetic data & controlled distributions\n",
    "- This is baseline + first innovation validation\n",
    "\n",
    "In real ML workflows:\n",
    "- Validate architecture correctness ✅ (done)\n",
    "- Validate training pipeline correctness ✅ (done)\n",
    "- Validate controlled generalization ✅ (done)\n",
    "- Then stress-test realism ❗ (next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb873027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0f166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647880c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f748d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c64a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d629c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08849d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8aa452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185eedc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
