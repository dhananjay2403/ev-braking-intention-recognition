{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2d5c01",
   "metadata": {},
   "source": [
    "## Multitask Learning\n",
    "\n",
    "Instead of compressing information (autoencoder), we add supervision, forcing the model to learn why braking happens by predicting both intention and intensity.\n",
    "\n",
    "Why Multitask Learning is the RIGHT pivot\n",
    "\n",
    "Your HARD dataset exposed the real issue:\n",
    "- Ambiguity between Light ↔ Normal braking\n",
    "- Labels depend on future behavior\n",
    "- Subtle temporal cues matter more than denoising\n",
    "\n",
    "Autoencoders failed because they removed information.\n",
    "\n",
    "Multitask learning does the opposite: It adds supervision, not compression.\n",
    "\n",
    "By asking the model to solve two related tasks at once, we force it to learn representations that explain why braking happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df5b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 75, 3)\n",
      "(10500,)\n",
      "(10500,)\n",
      "0.15013748255769468 0.9999489978076975\n"
     ]
    }
   ],
   "source": [
    "# Sanity check (for MTL data generator)\n",
    "import numpy as np\n",
    "\n",
    "X = np.load(\"../data/X_train_hard_mtl.npy\")\n",
    "y_c = np.load(\"../data/y_class_train_hard_mtl.npy\")\n",
    "y_i = np.load(\"../data/y_int_train_hard_mtl.npy\")\n",
    "\n",
    "print(X.shape)     # (N, 75, 3)\n",
    "print(y_c.shape)   # (N,)\n",
    "print(y_i.shape)   # (N,)\n",
    "print(y_i.min(), y_i.max())  # should be within [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cba3d5e",
   "metadata": {},
   "source": [
    "## Multitask Learning Architecture for Braking Intention Prediction\n",
    "\n",
    "### Motivation\n",
    "Experiments on the HARD ambiguous braking dataset revealed that:\n",
    "- Braking intention classes (Light vs Normal) overlap significantly\n",
    "- Labels depend on future braking behavior\n",
    "- Representation compression (autoencoders) degrades fine-grained temporal cues\n",
    "\n",
    "To address this, we adopt **multitask learning**, which provides additional task-aligned supervision instead of compressing information.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "The model is trained to jointly solve two related tasks:\n",
    "1. **Braking Intention Classification** (Light / Normal / Emergency)\n",
    "2. **Brake Intensity Regression** (future braking strength ∈ [0,1])\n",
    "\n",
    "By learning *what* the driver intends to do and *how strongly* they intend to brake, the model develops richer and more discriminative temporal representations.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Architecture Overview\n",
    "\n",
    "![Braking Intention Example](../img/1.jpg)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Shared Backbone\n",
    "- **Temporal CNN**: captures short-term local patterns (brake taps, fluctuations)\n",
    "- **LSTM**: models long-term temporal dependencies and future intention buildup\n",
    "- **Attention**: focuses on critical moments (onset and ramp-up of braking)\n",
    "\n",
    "This shared backbone learns task-agnostic temporal features.\n",
    "\n",
    "---\n",
    "\n",
    "### Task-Specific Heads\n",
    "\n",
    "#### 1. Braking Intention (Classification)\n",
    "- Output: 3 logits (Light, Normal, Emergency)\n",
    "- Activation: Softmax\n",
    "- Loss: CrossEntropyLoss\n",
    "\n",
    "#### 2. Brake Intensity (Regression)\n",
    "- Output: single continuous value\n",
    "- Represents future braking strength\n",
    "- Loss: Mean Squared Error (MSE)\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function\n",
    "The total training loss is a weighted sum:\n",
    "\n",
    "#### L_total = L_class + λ * L_reg\n",
    "\n",
    "Where:\n",
    "- Classification is the primary task\n",
    "- Regression acts as auxiliary supervision\n",
    "- λ is set to 0.5 initially\n",
    "\n",
    "---\n",
    "\n",
    "### Why Multitask Learning Works Here\n",
    "- Preserves subtle temporal details\n",
    "- Forces causal understanding of braking behavior\n",
    "- Reduces Light vs Normal ambiguity\n",
    "- Aligns learning objective with physical meaning\n",
    "\n",
    "This approach is better suited for ambiguous, future-dependent braking scenarios than representation compression methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01f3d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea99731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from models.multitask_lstm_cnn_attention import MultitaskLSTMCNNAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21cba210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Multitask HARD dataset\n",
    "X_train = np.load(\"../data/X_train_hard_mtl.npy\")\n",
    "X_val   = np.load(\"../data/X_val_hard_mtl.npy\")\n",
    "X_test  = np.load(\"../data/X_test_hard_mtl.npy\")\n",
    "\n",
    "y_class_train = np.load(\"../data/y_class_train_hard_mtl.npy\")\n",
    "y_class_val   = np.load(\"../data/y_class_val_hard_mtl.npy\")\n",
    "y_class_test  = np.load(\"../data/y_class_test_hard_mtl.npy\")\n",
    "\n",
    "y_int_train = np.load(\"../data/y_int_train_hard_mtl.npy\")\n",
    "y_int_val   = np.load(\"../data/y_int_val_hard_mtl.npy\")\n",
    "y_int_test  = np.load(\"../data/y_int_test_hard_mtl.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfc423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype = torch.float32)\n",
    "X_val_t   = torch.tensor(X_val, dtype = torch.float32)\n",
    "X_test_t  = torch.tensor(X_test, dtype = torch.float32)\n",
    "\n",
    "y_class_train_t = torch.tensor(y_class_train, dtype = torch.long)\n",
    "y_class_val_t   = torch.tensor(y_class_val, dtype = torch.long)\n",
    "y_class_test_t  = torch.tensor(y_class_test, dtype = torch.long)\n",
    "\n",
    "y_int_train_t = torch.tensor(y_int_train, dtype = torch.float32)\n",
    "y_int_val_t   = torch.tensor(y_int_val, dtype = torch.float32)\n",
    "y_int_test_t  = torch.tensor(y_int_test, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model & losses\n",
    "model = MultitaskLSTMCNNAttention()\n",
    "\n",
    "criterion_class = nn.CrossEntropyLoss()\n",
    "criterion_reg   = nn.MSELoss()\n",
    "\n",
    "lambda_reg = 0.8            # changed from 0.4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b0564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (multitask)\n",
    "def train_one_epoch_mtl(model, X, y_class, y_int, optimizer, batch_size):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    idx = torch.randperm(len(X))\n",
    "\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        batch_idx = idx[i : i+batch_size]\n",
    "\n",
    "        xb = X[batch_idx]\n",
    "        yb_class = y_class[batch_idx]\n",
    "        yb_int = y_int[batch_idx]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        class_logits, int_pred = model(xb)\n",
    "\n",
    "        loss_class = criterion_class(class_logits, yb_class)\n",
    "        loss_reg = criterion_reg(int_pred, yb_int)\n",
    "\n",
    "        loss = loss_class + lambda_reg * loss_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        preds = class_logits.argmax(dim = 1)\n",
    "        correct += (preds == yb_class).sum().item()\n",
    "\n",
    "    acc = correct / len(X)\n",
    "    return total_loss / len(X), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711dd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def evaluate_mtl(model, X, y_class, y_int):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        class_logits, int_pred = model(X)\n",
    "\n",
    "        loss_class = criterion_class(class_logits, y_class)\n",
    "        loss_reg = criterion_reg(int_pred, y_int)\n",
    "        loss = loss_class + lambda_reg * loss_reg\n",
    "\n",
    "        preds = class_logits.argmax(dim = 1)\n",
    "        correct = (preds == y_class).sum().item()\n",
    "\n",
    "    acc = correct / len(X)\n",
    "    return loss.item(), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2234bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MTL] Epoch 1/20 | Train Acc: 0.388 | Val Acc: 0.666\n",
      "[MTL] Epoch 2/20 | Train Acc: 0.680 | Val Acc: 0.678\n",
      "[MTL] Epoch 3/20 | Train Acc: 0.714 | Val Acc: 0.691\n",
      "[MTL] Epoch 4/20 | Train Acc: 0.704 | Val Acc: 0.677\n",
      "[MTL] Epoch 5/20 | Train Acc: 0.712 | Val Acc: 0.724\n",
      "[MTL] Epoch 6/20 | Train Acc: 0.718 | Val Acc: 0.720\n",
      "[MTL] Epoch 7/20 | Train Acc: 0.722 | Val Acc: 0.707\n",
      "[MTL] Epoch 8/20 | Train Acc: 0.721 | Val Acc: 0.726\n",
      "[MTL] Epoch 9/20 | Train Acc: 0.718 | Val Acc: 0.698\n",
      "[MTL] Epoch 10/20 | Train Acc: 0.724 | Val Acc: 0.681\n",
      "[MTL] Epoch 11/20 | Train Acc: 0.721 | Val Acc: 0.693\n",
      "[MTL] Epoch 12/20 | Train Acc: 0.724 | Val Acc: 0.711\n",
      "[MTL] Epoch 13/20 | Train Acc: 0.728 | Val Acc: 0.720\n",
      "[MTL] Epoch 14/20 | Train Acc: 0.724 | Val Acc: 0.716\n",
      "[MTL] Epoch 15/20 | Train Acc: 0.725 | Val Acc: 0.664\n",
      "[MTL] Epoch 16/20 | Train Acc: 0.726 | Val Acc: 0.688\n",
      "[MTL] Epoch 17/20 | Train Acc: 0.724 | Val Acc: 0.724\n",
      "[MTL] Epoch 18/20 | Train Acc: 0.727 | Val Acc: 0.666\n",
      "[MTL] Epoch 19/20 | Train Acc: 0.718 | Val Acc: 0.722\n",
      "[MTL] Epoch 20/20 | Train Acc: 0.728 | Val Acc: 0.722\n"
     ]
    }
   ],
   "source": [
    "# Train Multitask model\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc = train_one_epoch_mtl(\n",
    "        model,\n",
    "        X_train_t,\n",
    "        y_class_train_t,\n",
    "        y_int_train_t,\n",
    "        optimizer,\n",
    "        BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate_mtl(\n",
    "        model,\n",
    "        X_val_t,\n",
    "        y_class_val_t,\n",
    "        y_int_val_t\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"[MTL] Epoch {epoch+1}/{EPOCHS} | \"\n",
    "        f\"Train Acc: {train_acc:.3f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c8f297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MTL] Test Accuracy: 0.7102\n"
     ]
    }
   ],
   "source": [
    "# Test set evaluation \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    class_logits, _ = model(X_test_t)\n",
    "    preds = class_logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "test_acc = (preds == y_class_test).mean()\n",
    "\n",
    "print(f\"[MTL] Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948ee618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (MTL):\n",
      "[[593 199   2]\n",
      " [218 485 103]\n",
      " [  6 124 520]]\n",
      "\n",
      "Classification Report (MTL):\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    Light Braking       0.73      0.75      0.74       794\n",
      "   Normal Braking       0.60      0.60      0.60       806\n",
      "Emergency Braking       0.83      0.80      0.82       650\n",
      "\n",
      "         accuracy                           0.71      2250\n",
      "        macro avg       0.72      0.72      0.72      2250\n",
      "     weighted avg       0.71      0.71      0.71      2250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (MTL):\")\n",
    "print(confusion_matrix(y_class_test, preds))\n",
    "\n",
    "print(\"\\nClassification Report (MTL):\")\n",
    "print(classification_report(\n",
    "    y_class_test,\n",
    "    preds,\n",
    "    target_names=[\"Light Braking\", \"Normal Braking\", \"Emergency Braking\"]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be03cf2",
   "metadata": {},
   "source": [
    "Increasing λ:\n",
    "\n",
    "Forced the shared backbone to encode braking strength more explicitly\n",
    "\n",
    "Reduced reliance on brittle class boundaries\n",
    "\n",
    "Helped separate:\n",
    "- Light vs Normal\n",
    "- Normal vs Emergency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
